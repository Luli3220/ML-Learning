# 参数模型 vs 非参数模型：

## 参数模型（Parametric Models）

参数模型假设数据符合某种**固定结构**，只需学习**有限个参数**。

- **特点**：
  - 参数个数固定，不随训练数据增多而变化。
  - 假设函数形式已知，如线性或高斯分布。
- **优点**：
  - 模型训练速度快，内存消耗低。
  - 一旦训练好，预测效率高。
- **缺点**：
  - 假设过强，容易欠拟合，泛化能力可能较差。
---

### **参数模型（Parametric Models）**
参数模型假设数据服从某种特定的概率分布，并且通过估计有限数量的参数来描述这个分布。这类模型通常具有固定的参数空间。

#### 本书中的参数模型：
1. **线性回归**
   - 假设数据服从线性关系，通过估计权重参数来拟合数据。
2. **逻辑斯谛回归**
   - 假设输出概率服从 logistic 函数，通过估计权重参数来拟合数据。
3. **双线性模型**
   - 通常用于矩阵分解问题，通过估计固定数量的参数来建模。
4. **神经网络与多层感知机**
   - 虽然参数较多，但仍然是基于固定结构的参数化模型，通过调整权重参数进行学习。
5. **卷积神经网络**
   - 参数化模型，通过卷积核和全连接层的权重参数进行学习。
6. **循环神经网络**
   - 参数化模型，通过循环单元的权重参数进行学习。
7. **支持向量机（SVM）**
   - 在某些情况下（如使用核函数时），可以被视为非参数模型，但在标准形式下是参数化的，通过优化超平面的参数进行学习。
8. **主成分分析（PCA）**
   - 参数化模型，通过估计特征向量和方差等参数进行降维。
9. **概率图模型**
   - 参数化模型，通过估计节点之间的条件概率或联合概率分布的参数。
10. **EM算法**
    - EM 算法本身是一种迭代优化方法，常用于参数化模型的训练（如高斯混合模型）。
11. **自动编码器**
    - 参数化模型，通过编码器和解码器的权重参数进行学习。



---

## 非参数模型（Non-parametric Models）

非参数模型**不预设数据的函数形式**，结构灵活，参数个数可以随着数据的增加而增长。

- **特点**：
  - 模型容量随着数据量增大而增加。
  - 更适用于复杂、非线性的问题。
- **优点**：
  - 表达能力强，拟合复杂数据表现更好。
- **缺点**：
  - 训练和预测开销较大，计算复杂度高。
  - 容易过拟合。

#### 本书中的非参数模型：
1. **k近邻算法（KNN）**
   - 不依赖于数据的特定分布，而是直接基于最近邻的样本进行预测。
2. **决策树**
   - 决策树的结构可以根据数据动态生成，参数数量随树的深度和复杂度变化。
3. **集成学习与梯度提升决策树（GBDT）**
   - 集成多个决策树，每个树的结构是非参数化的。
4. **k均值聚类（K-means）**
   - 虽然需要指定簇的数量 k，但其参数数量随数据点的数量变化，因此可以视为非参数模型的一种变体。

---

## 对比总结表

| 特性            | 参数模型              | 非参数模型                |
|-----------------|-----------------------|---------------------------|
| 参数数量        | 固定                  | 随数据量变化               |
| 模型结构        | 假设已知（如线性）     | 不固定，自适应结构         |
| 训练速度        | 通常较快              | 较慢                      |
| 表达能力        | 可能受限               | 更强                      |
| 泛化能力        | 易欠拟合               | 易过拟合（需正则化）        |
| 典型算法        | 线性/逻辑回归、神经网络 | KNN、决策树、随机森林等     |

---

##  注意

- **参数模型适合小数据集或已知结构明确的问题**；
- **非参数模型适合结构复杂、数据丰富但先验假设不强的问题**。

