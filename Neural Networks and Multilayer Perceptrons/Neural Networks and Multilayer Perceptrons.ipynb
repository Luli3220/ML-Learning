{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多层感知机（MLP）\n",
    "\n",
    "## 1. 概述\n",
    "\n",
    "多层感知机（Multi-layer Perceptron, MLP）是一种前馈人工神经网络模型，由多个神经元层组成，包括输入层、隐藏层和输出层。每一层的神经元通过加权连接与下一层的神经元相连，并通过非线性激活函数引入非线性特性。MLP广泛应用于分类、回归和其他机器学习任务。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 基本结构\n",
    "\n",
    "一个典型的MLP由以下几部分组成：\n",
    "\n",
    "1. **输入层**：接收输入数据。\n",
    "2. **隐藏层**：包含多个神经元，用于提取特征。\n",
    "3. **输出层**：生成最终的预测结果。\n",
    "4. **激活函数**：引入非线性特性，例如Sigmoid、ReLU或Tanh。\n",
    "5. **权重和偏置**：控制神经元之间的连接强度。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 前向传播公式\n",
    "\n",
    "假设我们有一个MLP，其输入为 $\\mathbf{x} \\in \\mathbb{R}^d$，隐藏层有 $H$ 个神经元，输出层有 $K$ 个神经元。\n",
    "\n",
    "### 3.1 输入到隐藏层\n",
    "对于隐藏层的第 $j$ 个神经元，其计算公式为：\n",
    "$$\n",
    "z_j^{(1)} = \\sum_{i=1}^d w_{ji}^{(1)} x_i + b_j^{(1)}\n",
    "$$\n",
    "其中：\n",
    "- $w_{ji}^{(1)}$ 是从输入层第 $i$ 个神经元到隐藏层第 $j$ 个神经元的权重；\n",
    "- $b_j^{(1)}$ 是隐藏层第 $j$ 个神经元的偏置；\n",
    "- $z_j^{(1)}$ 是隐藏层第 $j$ 个神经元的加权输入。\n",
    "\n",
    "经过激活函数后，隐藏层的输出为：\n",
    "$$\n",
    "a_j^{(1)} = f(z_j^{(1)})\n",
    "$$\n",
    "其中 $f(\\cdot)$ 是激活函数。\n",
    "\n",
    "### 3.2 隐藏层到输出层\n",
    "对于输出层的第 $k$ 个神经元，其计算公式为：\n",
    "$$\n",
    "z_k^{(2)} = \\sum_{j=1}^H w_{kj}^{(2)} a_j^{(1)} + b_k^{(2)}\n",
    "$$\n",
    "其中：\n",
    "- $w_{kj}^{(2)}$ 是从隐藏层第 $j$ 个神经元到输出层第 $k$ 个神经元的权重；\n",
    "- $b_k^{(2)}$ 是输出层第 $k$ 个神经元的偏置；\n",
    "- $z_k^{(2)}$ 是输出层第 $k$ 个神经元的加权输入。\n",
    "\n",
    "经过激活函数后，输出层的输出为：\n",
    "$$\n",
    "a_k^{(2)} = g(z_k^{(2)})\n",
    "$$\n",
    "其中 $g(\\cdot)$ 是输出层的激活函数（例如Softmax用于分类任务，恒等函数用于回归任务）。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 反向传播算法\n",
    "\n",
    "反向传播（Backpropagation）是训练MLP的核心算法，用于计算损失函数相对于每个参数的梯度。以下是推导过程。\n",
    "\n",
    "### 4.1 损失函数\n",
    "定义损失函数 $L$，通常使用均方误差（MSE）或交叉熵损失：\n",
    "- MSE: $L = \\frac{1}{2} \\sum_{k=1}^K (y_k - a_k^{(2)})^2$\n",
    "- 交叉熵: $L = -\\sum_{k=1}^K y_k \\log(a_k^{(2)})$\n",
    "\n",
    "其中 $y_k$ 是目标值，$a_k^{(2)}$ 是预测值。\n",
    "\n",
    "### 4.2 输出层的梯度\n",
    "计算损失函数对输出层加权输入 $z_k^{(2)}$ 的梯度：\n",
    "$$\n",
    "\\delta_k^{(2)} = \\frac{\\partial L}{\\partial z_k^{(2)}} = \\frac{\\partial L}{\\partial a_k^{(2)}} \\cdot g'(z_k^{(2)})\n",
    "$$\n",
    "对于MSE：\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_k^{(2)}} = a_k^{(2)} - y_k\n",
    "$$\n",
    "对于交叉熵（Softmax激活）：\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_k^{(2)}} = a_k^{(2)} - y_k\n",
    "$$\n",
    "\n",
    "### 4.3 隐藏层的梯度\n",
    "计算损失函数对隐藏层加权输入 $z_j^{(1)}$ 的梯度：\n",
    "$$\n",
    "\\delta_j^{(1)} = \\frac{\\partial L}{\\partial z_j^{(1)}} = f'(z_j^{(1)}) \\sum_{k=1}^K \\delta_k^{(2)} w_{kj}^{(2)}\n",
    "$$\n",
    "\n",
    "### 4.4 参数更新\n",
    "根据梯度下降法更新权重和偏置：\n",
    "$$\n",
    "w_{ji}^{(1)} \\leftarrow w_{ji}^{(1)} - \\eta \\frac{\\partial L}{\\partial w_{ji}^{(1)}}\n",
    "$$\n",
    "$$\n",
    "b_j^{(1)} \\leftarrow b_j^{(1)} - \\eta \\frac{\\partial L}{\\partial b_j^{(1)}}\n",
    "$$\n",
    "$$\n",
    "w_{kj}^{(2)} \\leftarrow w_{kj}^{(2)} - \\eta \\frac{\\partial L}{\\partial w_{kj}^{(2)}}\n",
    "$$\n",
    "$$\n",
    "b_k^{(2)} \\leftarrow b_k^{(2)} - \\eta \\frac{\\partial L}{\\partial b_k^{(2)}}\n",
    "$$\n",
    "其中 $\\eta$ 是学习率。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 总结\n",
    "\n",
    "MLP通过多层神经元和非线性激活函数，能够逼近任意复杂的函数关系。反向传播算法利用链式法则高效地计算梯度，使得MLP可以通过梯度下降法进行训练。尽管MLP在理论上具有强大的表达能力，但在实际应用中需要注意过拟合、梯度消失等问题。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的，那我们来实现一下算法吧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先来看一下数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集的大小为: 1000\n",
      "[[ 1.7641  0.4002  0.    ]\n",
      " [ 0.9787  2.2409  0.    ]\n",
      " [ 1.8676 -0.9773  1.    ]\n",
      " [ 0.9501 -0.1514  1.    ]\n",
      " [-0.1032  0.4106  1.    ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#导入数据集\n",
    "data=np.loadtxt('xor_dataset.csv',delimiter=',')\n",
    "print('数据集的大小为:',len(data))\n",
    "print(data[:5])\n",
    "\n",
    "#划分数据集\n",
    "data=np.random.permutation(data)\n",
    "\n",
    "ratio=0.8\n",
    "split=int(ratio*len(data))\n",
    "np.random.seed(0)\n",
    "x_train,y_train=data[:split,:2],data[:split,2].reshape(-1,1)\n",
    "x_test,y_test=data[split:,:2],data[split:,2].reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`raise`\n",
    "\n",
    " 是 Python 中用于主动抛出异常的关键字。\n",
    "\n",
    "`NotImplementedError `\n",
    "\n",
    "是 Python 内置的一个异常类，表示某个方法或功能尚未实现。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用基类的原因是因为每一层的感知机的参数是一致的，就是数量不同\n",
    "\n",
    "所以我们用继承就行了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基类\n",
    "class Layer:\n",
    "  #向前传播函数，根据输入x计算该层的输出y\n",
    "  def forward(self,x):\n",
    "    raise NotImplementedError\n",
    "  #反向传播函数，输入上一层回传的梯度grad，输出当前层的梯度\n",
    "  def backward(self,grad):\n",
    "    raise NotImplementedError\n",
    "  #跟新函数,用于更新当前层的参数\n",
    "  def updata(self,learning_rate):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解释一下参数\n",
    "- `num_in` 输入维度\n",
    "- `num_out` 输出维度\n",
    "- `use_bias` 是否添加偏置\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一层的感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 感知机的前向传播与反向传播\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 参数维度说明\n",
    "\n",
    "在感知机中，输入数据、权重、偏置等参数的维度如下：\n",
    "\n",
    "- **输入数据**：$\\mathbf{x} \\in \\mathbb{R}^{\\text{batch\\_size} \\times \\text{num\\_in}}$\n",
    "  - $\\text{batch\\_size}$：批次大小。\n",
    "  - $\\text{num\\_in}$：输入特征的数量。\n",
    "\n",
    "- **权重矩阵**：$\\mathbf{W} \\in \\mathbb{R}^{\\text{num\\_in} \\times \\text{num\\_out}}$\n",
    "  - $\\text{num\\_out}$：输出神经元的数量。\n",
    "\n",
    "- **偏置向量**：$\\mathbf{b} \\in \\mathbb{R}^{1 \\times \\text{num\\_out}}$\n",
    "\n",
    "- **加权输入**：$\\mathbf{y} \\in \\mathbb{R}^{\\text{batch\\_size} \\times \\text{num\\_out}}$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 前向传播\n",
    "\n",
    "### 2.1 计算公式\n",
    "\n",
    "根据 `forward` 函数的实现，前向传播的过程如下：\n",
    "\n",
    "1. **线性变换**：\n",
    "   $$\n",
    "   \\mathbf{y} = \\mathbf{x} \\cdot \\mathbf{W}\n",
    "   $$\n",
    "   - 维度：$\\mathbf{y} \\in \\mathbb{R}^{\\text{batch\\_size} \\times \\text{num\\_out}}$\n",
    "\n",
    "2. **加偏置（如果使用偏置）**：\n",
    "   $$\n",
    "   \\mathbf{y} = \\mathbf{y} + \\mathbf{b}\n",
    "   $$\n",
    "   - 维度：$\\mathbf{y} \\in \\mathbb{R}^{\\text{batch\\_size} \\times \\text{num\\_out}}$\n",
    "\n",
    "3. **返回结果**：\n",
    "   - 输出 $\\mathbf{y}$ 的维度为 $(\\text{batch\\_size}, \\text{num\\_out})$。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 反向传播\n",
    "\n",
    "### 3.1 梯度计算公式\n",
    "\n",
    "反向传播的目标是根据损失函数 $L$ 的梯度 $\\frac{\\partial L}{\\partial \\mathbf{y}}$ 计算权重和偏置的梯度，并传播到前一层。\n",
    "\n",
    "#### 1. 权重梯度\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\mathbf{x}^\\top \\cdot \\text{grad}}{\\text{grad.shape}[0]}\n",
    "$$\n",
    "- 维度：$\\frac{\\partial L}{\\partial \\mathbf{W}} \\in \\mathbb{R}^{\\text{num\\_in} \\times \\text{num\\_out}}$\n",
    "- 其中：\n",
    "  - $\\mathbf{x}$ 是输入数据，维度为 $(\\text{batch\\_size}, \\text{num\\_in})$。\n",
    "  - $\\text{grad}$ 是从后一层传回的梯度，维度为 $(\\text{batch\\_size}, \\text{num\\_out})$。\n",
    "  - 归一化因子 $\\text{grad.shape}[0]$ 表示批次大小。\n",
    "\n",
    "#### 2. 偏置梯度（如果使用偏置）\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} = \\text{mean}(\\text{grad}, \\text{axis}=0, \\text{keepdims}=\\text{True})\n",
    "$$\n",
    "- 维度：$\\frac{\\partial L}{\\partial \\mathbf{b}} \\in \\mathbb{R}^{1 \\times \\text{num\\_out}}$\n",
    "- 其中：\n",
    "  - $\\text{grad}$ 的维度为 $(\\text{batch\\_size}, \\text{num\\_out})$。\n",
    "  - 对批次维度求均值，得到形状为 $(1, \\text{num\\_out})$。\n",
    "\n",
    "#### 3. 向前传播的梯度\n",
    "$$\n",
    "\\text{grad\\_prev} = \\text{grad} \\cdot \\mathbf{W}^\\top\n",
    "$$\n",
    "- 维度：$\\text{grad\\_prev} \\in \\mathbb{R}^{\\text{batch\\_size} \\times \\text{num\\_in}}$\n",
    "- 其中：\n",
    "  - $\\mathbf{W}^\\top$ 是权重矩阵的转置，维度为 $(\\text{num\\_out}, \\text{num\\_in})$。\n",
    "  - $\\text{grad}$ 的维度为 $(\\text{batch\\_size}, \\text{num\\_out})$。\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 参数更新\n",
    "\n",
    "#### 1. 更新权重\n",
    "$$\n",
    "\\mathbf{W} = \\mathbf{W} - \\eta \\cdot \\frac{\\partial L}{\\partial \\mathbf{W}}\n",
    "$$\n",
    "- 其中 $\\eta$ 是学习率。\n",
    "\n",
    "#### 2. 更新偏置（如果使用偏置）\n",
    "$$\n",
    "\\mathbf{b} = \\mathbf{b} - \\eta \\cdot \\frac{\\partial L}{\\partial \\mathbf{b}}\n",
    "$$\n",
    "- 其中 $\\eta$ 是学习率。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "  def __init__(self,num_in,num_out,use_bias=True):\n",
    "    self.num_in=num_in\n",
    "    self.num_out=num_out\n",
    "    self.use_bias=use_bias\n",
    "    #【注意】 参数的初始化非常重要\n",
    "    #如果把参数默认设置为0，会导致Wx=0,后续计算失去意义\n",
    "    #我们直接采用正态分布来初始化参数\n",
    "    self.W=np.random.normal(loc=0,scale=1.0,size=(num_in,num_out))  #loc表示均值，scale表示标准差\n",
    "    if use_bias:\n",
    "      self.b=np.zeros((1,num_out))\n",
    "  def forward(self,x):\n",
    "    #向前传播 y=xW+b\n",
    "    #x的维度为(batch_size,num_in)\n",
    "    self.x=x\n",
    "    self.y=x@self.W\n",
    "    if self.use_bias:\n",
    "      self.y+=self.b\n",
    "    return self.y\n",
    "  def backward(self, grad):\n",
    "    #反向传播,按照链式法则计算\n",
    "    #grad的维度为(batch_size,num_out)\n",
    "    #梯度要对batch取平均\n",
    "    #grad_W的维度要和W相同,为(num_in,num_out)\n",
    "    self.grad_W=self.x.T@grad/grad.shape[0]\n",
    "    if self.use_bias:\n",
    "      #grad_b的维度要和b相同为(1,num_out)\n",
    "      self.grad_b=np.mean(grad,axis=0,keepdims=True)\n",
    "    #向前传播的grad维度为(bach_size,num_in)\n",
    "    grad=grad@self.W.T\n",
    "    return grad\n",
    "  def updata(self, learning_rate):\n",
    "    self.W-=learning_rate*self.grad_W\n",
    "    if self.use_bias:\n",
    "      self.b-=learning_rate*self.grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#单位函数\n",
    "class Identity(Layer):\n",
    "  def forward(self, x):\n",
    "    return x\n",
    "  def backward(self, grad):\n",
    "    return grad\n",
    "#逻辑斯蒂函数\n",
    "class Sigmoid(Layer):\n",
    "  def forward(self ,x):\n",
    "    self.x=x\n",
    "    self.y=1/(+np.exp(-x))\n",
    "    return self.y\n",
    "  def backward(self, grad):\n",
    "    return grad*self.y*(1-self.y)\n",
    "#tanh函数\n",
    "class Tanh(Layer):\n",
    "  def forward(self, x):\n",
    "    self.x=x\n",
    "    self.y=np.tanh(x)\n",
    "    return self.y\n",
    "  def backward(self, grad):\n",
    "    return grad*(1-self.y**2)\n",
    "#ReLU函数\n",
    "class ReLU(Layer):\n",
    "  def forward(self, x):\n",
    "    self.x=x\n",
    "    self.y=np.maximum(x,0)\n",
    "    return self.y\n",
    "  def backward(self, grad):\n",
    "    return grad*(self.x>=0)\n",
    "# 存储所有激活函数和对应名称，方便索引\n",
    "activation_dict = { \n",
    "    'identity': Identity,\n",
    "    'sigmoid': Sigmoid,\n",
    "    'tanh': Tanh,\n",
    "    'relu': ReLU\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
