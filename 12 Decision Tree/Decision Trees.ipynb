{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90225ac4",
   "metadata": {},
   "source": [
    "### 决策树\n",
    "\n",
    "决策树是一种常用的机器学习算法，用于分类和回归任务。ID3 和 C4.5 是两种经典的决策树算法，它们的核心思想是通过信息增益或信息增益率来选择最优的特征进行分裂，从而构建一棵决策树。以下是 ID3 和 C4.5 算法的总结，包括它们的原理、区别以及示例。\n",
    "\n",
    "---\n",
    "\n",
    "## **1. ID3 算法**\n",
    "\n",
    "### **1.1 基本原理**\n",
    "ID3（Iterative Dichotomiser 3）算法是最早提出的决策树算法之一，由 Ross Quinlan 提出。它基于**信息增益（Information Gain）**作为分裂标准，选择能够最大程度减少数据集不确定性（熵）的特征进行分裂。\n",
    "\n",
    "#### **关键概念：信息熵（Entropy）**\n",
    "信息熵是衡量数据集纯度的一个指标，公式如下：\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^{n} P(X=i) \\log P(X=i)\n",
    "$$\n",
    "其中，$P(X=i)$ 是类别 $i$ 的概率。\n",
    "\n",
    "#### **条件熵（Conditional Entropy）**\n",
    "条件熵表示在给定某个特征值的情况下，数据集的不确定性：\n",
    "$$\n",
    "H(X|Y) = -\\sum_{j=1}^{m} P(Y=j) \\sum_{i=1}^{n} P(X=i|Y=j) \\log P(X=i|Y=j)\n",
    "$$\n",
    "\n",
    "#### **信息增益（Information Gain）**\n",
    "信息增益是使用某个特征进行分裂后，数据集不确定性减少的程度：\n",
    "$$\n",
    "I(X, Y) = H(X) - H(X|Y)\n",
    "$$\n",
    "\n",
    "### **1.2 构建决策树的过程**\n",
    "1. 计算当前数据集的熵 $H(X)$。\n",
    "2. 对每个特征 $Y$，计算其条件熵 $H(X|Y)$。\n",
    "3. 计算每个特征的信息增益 $I(X, Y)$。\n",
    "4. 选择信息增益最大的特征作为当前节点的分裂特征。\n",
    "5. 根据该特征的不同取值，将数据集划分为子集。\n",
    "6. 对每个子集递归地重复上述过程，直到满足停止条件（如所有样本属于同一类、特征用尽等）。\n",
    "\n",
    "### **1.3 示例**\n",
    "假设我们有一个数据集，包含 14 个样本，目标是预测是否外出活动（$X=0$ 表示未外出，$X=1$ 表示外出）。特征包括天气（晴天 $Y_S$、阴天 $Y_O$、雨天 $Y_R$）和温度（高温 $Y_H$、适中 $Y_M$、低温 $Y_C$）。\n",
    "\n",
    "#### **步骤 1：计算原始数据的熵**\n",
    "$$\n",
    "H(X) = -\\frac{1}{2} \\log \\frac{1}{2} - \\frac{1}{2} \\log \\frac{1}{2} = 0.5 + 0.5 = 1\n",
    "$$\n",
    "\n",
    "#### **步骤 2：计算天气特征的信息增益**\n",
    "- 晴天 ($Y_S$)：5 个样本，外出 2 个，未外出 3 个：\n",
    "  $$\n",
    "  H(X|Y_S) = -\\frac{3}{5} \\log \\frac{3}{5} - \\frac{2}{5} \\log \\frac{2}{5} \\approx 0.9710\n",
    "  $$\n",
    "- 阴天 ($Y_O$)：4 个样本，外出 4 个，未外出 0 个：\n",
    "  $$\n",
    "  H(X|Y_O) = -\\frac{4}{4} \\log \\frac{4}{4} = 0\n",
    "  $$\n",
    "- 雨天 ($Y_R$)：5 个样本，外出 1 个，未外出 4 个：\n",
    "  $$\n",
    "  H(X|Y_R) = -\\frac{4}{5} \\log \\frac{4}{5} - \\frac{1}{5} \\log \\frac{1}{5} \\approx 0.7219\n",
    "  $$\n",
    "\n",
    "条件熵：\n",
    "$$\n",
    "H(X|Y) \\approx \\frac{5}{14} \\times 0.9710 + \\frac{4}{14} \\times 0 + \\frac{5}{14} \\times 0.7219 \\approx 0.6046\n",
    "$$\n",
    "\n",
    "信息增益：\n",
    "$$\n",
    "I(X, Y) = H(X) - H(X|Y) \\approx 1 - 0.6046 \\approx 0.3954\n",
    "$$\n",
    "\n",
    "#### **步骤 3：计算温度特征的信息增益**\n",
    "- 高温 ($Y_H$)：4 个样本，外出 2 个，未外出 2 个：\n",
    "  $$\n",
    "  H(X|Y_H) = -\\frac{2}{4} \\log \\frac{2}{4} - \\frac{2}{4} \\log \\frac{2}{4} = 1\n",
    "  $$\n",
    "- 适中 ($Y_M$)：4 个样本，外出 3 个，未外出 1 个：\n",
    "  $$\n",
    "  H(X|Y_M) = -\\frac{1}{4} \\log \\frac{1}{4} - \\frac{3}{4} \\log \\frac{3}{4} \\approx 0.8113\n",
    "  $$\n",
    "- 低温 ($Y_C$)：6 个样本，外出 2 个，未外出 4 个：\n",
    "  $$\n",
    "  H(X|Y_C) = -\\frac{4}{6} \\log \\frac{4}{6} - \\frac{2}{6} \\log \\frac{2}{6} \\approx 0.9183\n",
    "  $$\n",
    "\n",
    "条件熵：\n",
    "$$\n",
    "H(X|Y) \\approx \\frac{4}{14} \\times 1 + \\frac{4}{14} \\times 0.8113 + \\frac{6}{14} \\times 0.9183 \\approx 0.9111\n",
    "$$\n",
    "\n",
    "信息增益：\n",
    "$$\n",
    "I(X, Y) = H(X) - H(X|Y) \\approx 1 - 0.9111 \\approx 0.0889\n",
    "$$\n",
    "\n",
    "#### **结论**\n",
    "天气特征的信息增益为 $0.3954$，温度特征的信息增益为 $0.0889$。因此，ID3 算法会选择天气作为根节点。\n",
    "\n",
    "---\n",
    "\n",
    "## **2. C4.5 算法**\n",
    "\n",
    "### **2.1 基本原理**\n",
    "C4.5 是 ID3 的改进版本，由 Ross Quinlan 提出。它引入了**信息增益率（Information Gain Ratio）**作为分裂标准，以解决 ID3 中的一个问题：即某些特征可能具有大量取值（如唯一标识符），导致信息增益较大但实际意义不大。\n",
    "\n",
    "#### **信息增益率（Information Gain Ratio）**\n",
    "信息增益率是信息增益与特征自身熵的比值，公式如下：\n",
    "$$\n",
    "I_R(X, Y) = \\frac{I(X, Y)}{H_Y(X)}\n",
    "$$\n",
    "其中，$H_Y(X)$ 是特征 $Y$ 的熵，定义为：\n",
    "$$\n",
    "H_Y(X) = -\\sum_{y \\in Y} \\frac{|X_{Y=y}|}{|X|} \\log \\frac{|X_{Y=y}|}{|X|}\n",
    "$$\n",
    "\n",
    "#### **优点**\n",
    "- 解决了 ID3 中对高基数特征（如唯一标识符）过于敏感的问题。\n",
    "- 能够更好地处理连续型特征（通过离散化处理）。\n",
    "- 支持剪枝操作，避免过拟合。\n",
    "\n",
    "### **2.2 构建决策树的过程**\n",
    "1. 计算当前数据集的熵 $H(X)$。\n",
    "2. 对每个特征 $Y$，计算其信息增益 $I(X, Y)$ 和特征熵 $H_Y(X)$。\n",
    "3. 计算每个特征的信息增益率 $I_R(X, Y)$。\n",
    "4. 选择信息增益率最大的特征作为当前节点的分裂特征。\n",
    "5. 根据该特征的不同取值，将数据集划分为子集。\n",
    "6. 对每个子集递归地重复上述过程，直到满足停止条件。\n",
    "\n",
    "### **2.3 示例**\n",
    "继续使用上述天气和温度的例子，计算信息增益率。\n",
    "\n",
    "#### **步骤 1：计算天气特征的信息增益率**\n",
    "- 特征熵：\n",
    "  $$\n",
    "  H_Y(X) = -\\frac{5}{14} \\log \\frac{5}{14} - \\frac{4}{14} \\log \\frac{4}{14} - \\frac{5}{14} \\log \\frac{5}{14} \\approx 1.5774\n",
    "  $$\n",
    "- 信息增益率：\n",
    "  $$\n",
    "  I_R(X, Y) = \\frac{I(X, Y)}{H_Y(X)} \\approx \\frac{0.3954}{1.5774} \\approx 0.2507\n",
    "  $$\n",
    "\n",
    "#### **步骤 2：计算温度特征的信息增益率**\n",
    "- 特征熵：\n",
    "  $$\n",
    "  H_Y(X) = -\\frac{4}{14} \\log \\frac{4}{14} - \\frac{4}{14} \\log \\frac{4}{14} - \\frac{6}{14} \\log \\frac{6}{14} \\approx 1.5567\n",
    "  $$\n",
    "- 信息增益率：\n",
    "  $$\n",
    "  I_R(X, Y) = \\frac{I(X, Y)}{H_Y(X)} \\approx \\frac{0.0889}{1.5567} \\approx 0.0571\n",
    "  $$\n",
    "\n",
    "#### **结论**\n",
    "天气特征的信息增益率为 $0.2507$，温度特征的信息增益率为 $0.0571$。因此，C4.5 算法也会选择天气作为根节点。\n",
    "\n",
    "\n",
    "## 3 **正则化优化在决策树中的应用**\n",
    "\n",
    "在构建决策树时，尤其是像 ID3 和 C4.5 这样的算法，容易出现过拟合问题。为了缓解这一问题，可以引入**正则化（Regularization）**技术，通过在代价函数中加入复杂度惩罚项，平衡模型的精度和复杂度。以下是正则化优化的具体介绍，结合公式和示例进行说明。\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 决策树的过拟合问题**\n",
    "\n",
    "决策树算法在训练过程中会不断分裂节点，直到满足停止条件（如所有样本属于同一类、特征用尽等）。这种无限细化的过程可能导致以下问题：\n",
    "1. **高复杂度**：生成的决策树过于复杂，包含大量分支。\n",
    "2. **过拟合**：模型对训练数据拟合得非常好，但在测试数据上的表现较差。\n",
    "\n",
    "为了解决这些问题，需要在模型的精度和复杂度之间找到平衡，而正则化是一种有效的手段。\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 正则化优化的基本思想**\n",
    "\n",
    "正则化的核心思想是引入一个**复杂度惩罚项**，使得模型在追求分类精度的同时，不会过度复杂化。具体来说：\n",
    "- **精度部分**：衡量决策树的分类能力，通常使用叶节点的熵或基尼指数。\n",
    "- **复杂度部分**：惩罚决策树的复杂度，例如叶节点的数量。\n",
    "\n",
    "通过调整这两个部分的权重，可以在精度和复杂度之间取得平衡。\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 代价函数的设计**\n",
    "\n",
    "在决策树中，可以通过定义一个代价函数 $C(T)$ 来实现正则化优化。该函数同时考虑了模型的精度和复杂度：\n",
    "\n",
    "$$\n",
    "C(T) = \\sum_{t=1}^{|T|} N_t H_t(T) + \\lambda |T|\n",
    "$$\n",
    "\n",
    "#### **3.1 各项解释**\n",
    "1. **第一项：$\\sum_{t=1}^{|T|} N_t H_t(T)$**\n",
    "   - 表示所有叶节点的熵的加权和。\n",
    "   - $N_t$ 是叶节点 $t$ 上的样本数，$H_t(T)$ 是叶节点 $t$ 的熵。\n",
    "   - 熵 $H_t(T)$ 的公式为：\n",
    "     $$\n",
    "     H_t(T) = -\\sum_k \\frac{N_{tk}}{N_t} \\log \\frac{N_{tk}}{N_t}\n",
    "     $$\n",
    "     其中，$N_{tk}$ 是叶节点 $t$ 上类别为 $k$ 的样本数。\n",
    "   - 这一项反映了决策树的分类精度：熵越小，分类越精确。\n",
    "\n",
    "2. **第二项：$\\lambda |T|$**\n",
    "   - 表示叶节点的数量 $|T|$ 的惩罚项。\n",
    "   - $\\lambda \\geq 0$ 是正则化系数，用于控制复杂度惩罚的强度。\n",
    "   - 当 $\\lambda$ 较大时，模型更倾向于减少叶节点数量，从而降低复杂度；当 $\\lambda$ 较小时，模型更关注分类精度。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d839928",
   "metadata": {},
   "source": [
    "## 基于决策树的泰塔尼克号生还分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc13d9",
   "metadata": {},
   "source": [
    "#### 数据集:泰坦尼克号的生存预测数据集，它包含了许多泰坦尼克号上的乘客的特征信息，以及该乘客最后是否生还。\n",
    "\n",
    "由于编号，姓名，船票的编号都是唯一的，对建立模型的帮助很小，所以我们在构建数据集的时候优先将他们去掉\n",
    "\n",
    "\n",
    "| 列编号 | 特征名称       | 含义                     | 取值                   |\n",
    "|--------|----------------|--------------------------|------------------------|\n",
    "| 0      | 'PassengerId'(删除)  | 编号，从1开始            | 整数                   |\n",
    "| 1      | 'Survived'     | 是否生还，1代表生还，0代表遇难 | 0、1                  |\n",
    "| 2      | 'Pclass'       | 舱位等级                 | 0、1、2                |\n",
    "| 3      | 'Name'(删除)         | 姓名                     | 字符串                 |\n",
    "| 4      | 'Sex'          | 性别                     | 'male'、'female'       |\n",
    "| 5      | 'Age'          | 年龄                     | 浮点数                 |\n",
    "| 6      | 'SibSp'        | 登船的兄弟姐妹数量       | 整数                   |\n",
    "| 7      | 'Parch'        | 登船的父母和子女数量     | 整数                   |\n",
    "| 8      | 'Ticket'(删除)       | 船票编号                 | 字符串                 |\n",
    "| 9      | 'Fare'         | 船票价格                 | 浮点数                 |\n",
    "| 10     | 'Cabin'        | 所在船舱编号             | 字符串                 |\n",
    "| 11     | 'Embarked'     | 登船港口，C代表瑟堡，S代表南安普敦，Q代表昆斯敦 | 'C'、'S'、'Q'         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26783470",
   "metadata": {},
   "source": [
    "## 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43963ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa34727",
   "metadata": {},
   "source": [
    "## 数据集读取及其划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2596e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Cabin Embarked\n",
      "0         0       3    male  22.0      1      0   7.2500   NaN        S\n",
      "1         1       1  female  38.0      1      0  71.2833   C85        C\n",
      "2         1       3  female  26.0      0      0   7.9250   NaN        S\n",
      "3         1       1  female  35.0      1      0  53.1000  C123        S\n",
      "4         0       3    male  35.0      0      0   8.0500   NaN        S\n"
     ]
    }
   ],
   "source": [
    "#读取数据\n",
    "data=pd.read_csv('titanic/train.csv')\n",
    "\n",
    "#查看数据信息和前五行具体内容，其中NaN代表数据缺失\n",
    "print(data.info())\n",
    "print(data[:5])\n",
    "\n",
    "#删去编号\n",
    "#columns=['PassengerId', 'Name', 'Ticket']: 指定要删除的列名。\n",
    "#inplace=True: 表示直接在原 DataFrame 上修改，而不是返回一个新的 DataFrame。\n",
    "\n",
    "data.drop(columns=['PassengerId','Name','Ticket'],inplace=True)\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f928a7",
   "metadata": {},
   "source": [
    "## 数据集处理:\n",
    "  - ### 连续取值离散化\n",
    "  \n",
    "  - ### 分类类型转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23809f5e",
   "metadata": {},
   "source": [
    "在上面的算法介绍中，我们只考虑了特征取离散值的情况。在实践中，还有许多特征的取值是连续的，例如本数据集中的年龄和船票价格两项。\n",
    "\n",
    "对于这样的特征，我们可以根据数据的范围划出几个分类点 $x_1, \\dots, x_K$，并按照取值与分类点的大小关系进行分类。\n",
    "\n",
    "具体来说，可以将数据分成以下 $K+1$ 类：\n",
    "\n",
    "- $(-\\infty, x_1]$\n",
    "- $(x_1, x_2]$\n",
    "- $\\dots$\n",
    "- $(x_K, +\\infty)$\n",
    "\n",
    "这样，我们就把连续的数据转化成了离散的取值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1409b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age ：\n",
      "0.4200\n",
      "9.2622\n",
      "18.1044\n",
      "26.9467\n",
      "35.7889\n",
      "44.6311\n",
      "53.4733\n",
      "62.3156\n",
      "71.1578\n",
      "80.0000\n",
      "Fare ：\n",
      "0.0000\n",
      "56.9255\n",
      "113.8509\n",
      "170.7764\n",
      "227.7019\n",
      "284.6273\n",
      "341.5528\n",
      "398.4783\n",
      "455.4037\n",
      "512.3292\n"
     ]
    }
   ],
   "source": [
    "feat_ranges = {} #字典\n",
    "cont_feat = ['Age', 'Fare'] # 连续特征\n",
    "bins = 10 # 分类点数\n",
    "\n",
    "for feat in cont_feat:\n",
    "    # 数据集中存在缺省值nan，需要用np.nanmin和np.nanmax,如果直接使用 min() 或 max()，当列中存在NaN时会抛出错误或返回 NaN\n",
    "    # 计算当前特征列的最小值和最大值，忽略缺失值（NaN）。\n",
    "    min_val = np.nanmin(data[feat])   \n",
    "    max_val = np.nanmax(data[feat])   \n",
    "    feat_ranges[feat] = np.linspace(min_val, max_val, bins).tolist()  #.tolist()将NumPy数组转换为Python列表，方便后续操作。\n",
    "    print(feat, '：') # 查看分类点\n",
    "    for spt in feat_ranges[feat]:\n",
    "        print(f'{spt:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e6acc",
   "metadata": {},
   "source": [
    "#### pandas的分类类型\n",
    "\n",
    "分类类型是指数据的取值范围是有限的、离散的集合，而不是连续的数值。例如：\n",
    "\n",
    "- 性别 ：`['male', 'female']`\n",
    "- 颜色 ：`['red', 'green', 'blue']` \n",
    "\n",
    "\n",
    "分类类型的常见操作\n",
    "\n",
    " - 数据类型转换\n",
    "   \n",
    "   - 在 Pandas 中，可以将分类数据转换为 category 类型\n",
    "\n",
    "    ``` python\n",
    "    data['Sex'] = data['Sex'].astype('category')\n",
    "    ```\n",
    " - 查看类别\n",
    "\n",
    "   - 使用 `cat.categories` 查看所有类别：\n",
    "\n",
    "   ``` python \n",
    "   print(data['Sex'].cat.categories) \n",
    "   #输出示例\n",
    "   # Index(['female', 'male'], dtype='object')\n",
    "   ```\n",
    " \n",
    " - 编码\n",
    "   \n",
    "   - 整数编码（Label Encoding）\n",
    "     \n",
    "     - 将类别按顺序映射为整数：\n",
    "     ``` python\n",
    "     data['Sex'] = data['Sex'].cat.codes\n",
    "     #输出示例\n",
    "     #female -> 0 male -> 1\n",
    "     ```\n",
    "   \n",
    "   - 独热编码（One-Hot Encoding）\n",
    "\n",
    "    - 使用 Pandas 的 get_dummies 方法：\n",
    "     ``` python\n",
    "     data = pd.get_dummies(data, columns=['Sex'])\n",
    "     #输出示例\n",
    "     #   Sex_female  Sex_male\n",
    "     #0      0         1\n",
    "     #1      1         0\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3569e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex:Index(['female', 'male'], dtype='object')\n",
      "Pclass:Index([1, 2, 3], dtype='int64')\n",
      "SibSp:Index([0, 1, 2, 3, 4, 5, 8], dtype='int64')\n",
      "Parch:Index([0, 1, 2, 3, 4, 5, 6], dtype='int64')\n",
      "Cabin:Index(['A10', 'A14', 'A16', 'A19', 'A20', 'A23', 'A24', 'A26', 'A31', 'A32',\n",
      "       ...\n",
      "       'E8', 'F E69', 'F G63', 'F G73', 'F2', 'F33', 'F38', 'F4', 'G6', 'T'],\n",
      "      dtype='object', length=147)\n",
      "Embarked:Index(['C', 'Q', 'S'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 只有有限取值的离散特征\n",
    "cat_feat = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Cabin', 'Embarked'] \n",
    "\n",
    "for feat in cat_feat:\n",
    "    data[feat]=data[feat].astype('category') # 将特征列的数据类型转换为Pandas的category类型\n",
    "    print(f'{feat}:{data[feat].cat.categories}') #查看类别\n",
    "    data[feat]=data[feat].cat.codes.to_list() #将类别按顺序转化为整数\n",
    "    ranges=list(set(data[feat]))  #使用 set(data[feat]) 获取当前特征的所有唯一取值。\n",
    "    ranges.sort()             \n",
    "    feat_ranges[feat]=ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b16fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将所有缺省值替换为-1\n",
    "data.fillna(-1,inplace=True)  \n",
    "for feat in feat_ranges.keys():  # 遍历字典feat_ranges中的所有键（即特征名）。\n",
    "    feat_ranges[feat]=[-1]+feat_ranges[feat] #使用列表拼接操作[-1] + feat_ranges[feat]，确保 -1 成为第一个元素。\n",
    "\n",
    "#这行代码的作用是将 -1 添加到每个特征的取值范围中，确保缺失值的填充值也被视为合法的类别之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4786da",
   "metadata": {},
   "source": [
    "## 划分数据集与测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked'], dtype='object')\n",
      "Survived\n",
      "训练集大小: 712\n",
      "测试集大小: 179\n",
      "特征数大小: 8\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "feat_names=data.columns[1:]\n",
    "label_name=data.columns[0]\n",
    "print(feat_names)\n",
    "print(label_name)\n",
    "\n",
    "#重排下标\n",
    "data=data.reindex(np.random.permutation(data.index))\n",
    "ratio=0.8\n",
    "split=int(ratio*len(data))\n",
    "\n",
    "train_x=data[:split].drop(columns='Survived').to_numpy()  #pandas转numpy\n",
    "train_y=data['Survived'][:split].to_numpy()\n",
    "test_x=data[split:].drop(columns='Survived').to_numpy()\n",
    "test_y=data['Survived'][split:].to_numpy()\n",
    "\n",
    "print('训练集大小:',len(train_x))\n",
    "print('测试集大小:',len(test_x))\n",
    "print('特征数大小:',train_x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc6f1a",
   "metadata": {},
   "source": [
    "## 基类（结点） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d85b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        #内部结点的feat表示用来分类的特征编号，其数字与数据中的顺序对应\n",
    "        #叶节点的feat表示该结点的分类结果\n",
    "        self.feat=None\n",
    "        #分类值列表，表示按照其中的值向子结点分类\n",
    "        self.split=None\n",
    "        #子节点列表，叶节点的child为空\n",
    "        self.child=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self,X,Y,feat_ranges,lbd):\n",
    "        self.root=Node()\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "        self.feat_ranges=feat_ranges #特征取值范围\n",
    "        self.lbd=lbd  #正则化系数\n",
    "        self.eps=1e-8 #防止log0的情况\n",
    "        self.T=0  #记录叶节点数\n",
    "        self.ID3(self.root,self.X,self.Y)\n",
    "\n",
    "    #工具函数，计算 a*log a\n",
    "    def aloga(self,a):\n",
    "        return a*np.log2(a+self.eps)\n",
    "    \n",
    "    #计算某个子数据集的熵\n",
    "    def entropy(self,Y):\n",
    "        #统计每个类别出现的次数\n",
    "        #返回的是一个包含两个元素的元组：\n",
    "        # 0唯一值  1 每个唯一值对应的出现次数 2数据类型\n",
    "        cnt=np.unique(Y,return_counts=True)[1]   #return_counts=True: 表示返回每个唯一值的出现次数。 \n",
    "        N=len(Y)\n",
    "        ent=-np.sum([self.aloga(Ni/N) for Ni in cnt])\n",
    "        return ent\n",
    "    # np.unique 示例 ： a=[0,1,0,1] cnt=np.unique(a,return_counts=True)\n",
    "    # 输出: (array([0, 1]), array([2, 2], dtype=int64))\n",
    "    \n",
    "    #计算feat<=val划分数据集的信息增益\n",
    "    def info_gain(self,X,Y,feat,val):\n",
    "        #划分前的熵\n",
    "        N=len(Y)\n",
    "        if N==0 :\n",
    "            return 0\n",
    "        HX=self.entropy(Y)\n",
    "        HXY=0 #H（X|Y）\n",
    "        #分别计算H（X|X_F<=val）和分别计算H（X|X_F>val）\n",
    "        Y_l=Y[X[:,feat]<=val]\n",
    "        HXY+=len(Y_l)/len(Y)*self.entropy(Y_l)\n",
    "        Y_r=Y[X[:,feat]>val]\n",
    "        HXY+=len(Y_r)/len(Y)*self.entropy(Y_r)\n",
    "        return HX-HXY\n",
    "    \n",
    "    #计算特征feat<=val本身的复杂度H_Y(X)\n",
    "    def entropy_YX(self,X,Y,feat,val):\n",
    "        HYX = 0\n",
    "        N = len(Y)\n",
    "        if N == 0:\n",
    "            return 0\n",
    "        Y_l = Y[X[:, feat] <= val]\n",
    "        HYX += -self.aloga(len(Y_l) / N)\n",
    "        Y_r = Y[X[:, feat] > val]\n",
    "        HYX += -self.aloga(len(Y_r) / N)\n",
    "        return HYX\n",
    "    \n",
    "    #计算用feat<=val划分数据集的信息增益率\n",
    "    def info_gain_ratio(self,X,Y,feat,val):\n",
    "        IG=self.info_gain(X,Y,feat,val)\n",
    "        HYX=self.entropy_YX(X,Y,feat,val)\n",
    "        return IG/HYX\n",
    "        \n",
    "    #用ID3算法递归分裂结点，构造决策树\n",
    "    def ID3(self,node,X,Y):\n",
    "        #判断是否已经分类完成\n",
    "        if len(np.unique(Y))==1:\n",
    "            node.feat=Y[0]\n",
    "            self.T+=1\n",
    "            return \n",
    "        \n",
    "        #寻找最优分类特征和分类点\n",
    "        best_IGR=0\n",
    "        best_feat=None\n",
    "        best_val=None\n",
    "        for feat in range(len(feat_names)):\n",
    "            for val in self.feat_ranges[feat_names[feat]]:\n",
    "                IGR=self.info_gain_ratio(X,Y,feat,val)\n",
    "                if IGR>best_IGR:\n",
    "                    best_IGR=IGR\n",
    "                    best_feat=feat\n",
    "                    best_val=val\n",
    "        #计算用best_feat<=best_val分类带来的代价函数变化\n",
    "        #由于分裂叶结点只涉及该局部，我们只需要计算分裂前后该结点的代价函数\n",
    "        \n",
    "        #当前代价\n",
    "        cur_cost=len(Y)*self.entropy(Y)+self.lbd\n",
    "        \n",
    "        #分裂后的代价，按best_feat的取值分裂统计\n",
    "        #如果best_feat为None  说明(1)数据集中所有样本已经属于同一类别。(2)信息增益率为 0，无法通过分裂进一步减少不确定性。\n",
    "        #在分类也无法增加信息了，因此将new_cost设置为无穷大表示在这种情况下不进行分裂\n",
    "        if best_feat is None:\n",
    "            new_cost=np.inf\n",
    "        else:\n",
    "            new_cost=0\n",
    "            X_feat=X[:,best_feat]\n",
    "            #获取划分后的两部分，计算新的熵\n",
    "            new_Y_l=Y[X_feat<=best_val]\n",
    "            new_cost+=len(new_Y_l)*self.entropy(new_Y_l)\n",
    "            new_Y_r=Y[X_feat>best_val]\n",
    "            new_cost+=len(new_Y_r)*self.entropy(new_Y_r)\n",
    "            #分裂后会有两个叶节点   【注意】lbd表示对每个叶节点的惩罚力度,用途是为什么减少节点数\n",
    "            new_cost+=2*self.lbd\n",
    "        \n",
    "        #  如果分裂后代价更小，那么执行分裂\n",
    "        if new_cost <= cur_cost:\n",
    "            node.feat = best_feat\n",
    "            node.split = best_val\n",
    "            l_child = Node()\n",
    "            l_X = X[X_feat <= best_val]\n",
    "            l_Y = Y[X_feat <= best_val]\n",
    "            self.ID3(l_child, l_X, l_Y)\n",
    "            r_child = Node()\n",
    "            r_X = X[X_feat > best_val]\n",
    "            r_Y = Y[X_feat > best_val]\n",
    "            self.ID3(r_child, r_X, r_Y)\n",
    "            node.child = [l_child, r_child]\n",
    "        else:\n",
    "            # 否则将当前结点上最多的类别作为该结点的类别\n",
    "            vals, cnt = np.unique(Y, return_counts=True)\n",
    "            node.feat = vals[np.argmax(cnt)]\n",
    "            self.T += 1\n",
    "        \n",
    "    def predict(self,x):\n",
    "        node=self.root\n",
    "        #从根结点开始向下寻找，找到叶结点结束\n",
    "        while node.split is not None:\n",
    "            if x[node.feat] <= node.split:\n",
    "                node=node.child[0]\n",
    "            else:\n",
    "                node=node.child[1]\n",
    "        \n",
    "        #到达叶结点，返回类型\n",
    "        return node.feat\n",
    "    \n",
    "    #计算在样本X，标签Y上的准确率\n",
    "    def accuracy(self,X,Y):\n",
    "        correct=0\n",
    "        for x,y in zip(X,Y):\n",
    "            pred=self.predict(x)\n",
    "            if pred==y:\n",
    "                correct+=1\n",
    "        return correct/len(Y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28fef007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "叶结点数量： 23\n",
      "训练集准确率： 0.8300561797752809\n",
      "测试集准确率： 0.7262569832402235\n"
     ]
    }
   ],
   "source": [
    "DT = DecisionTree(train_x, train_y, feat_ranges, lbd=1.0)\n",
    "print('叶结点数量：', DT.T)\n",
    "\n",
    "# 计算在训练集和测试集上的准确率\n",
    "print('训练集准确率：', DT.accuracy(train_x, train_y))\n",
    "print('测试集准确率：', DT.accuracy(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aedfd5",
   "metadata": {},
   "source": [
    "## Sklearn 中的 Tree 模型\n",
    "\n",
    "Scikit-learn（简称 `sklearn`）是 Python 中一个功能强大的机器学习库，提供了多种机器学习算法的实现。其中，`tree` 模块专注于基于决策树的模型，包括分类和回归任务。本文将详细介绍 `sklearn.tree` 的核心内容及其使用方法。\n",
    "\n",
    "---\n",
    "\n",
    "##   `sklearn.tree` 模块的核心类\n",
    "\n",
    "`sklearn.tree` 提供了以下主要类来构建和使用决策树模型：\n",
    "\n",
    "### 1 `DecisionTreeClassifier`\n",
    "- **用途**: 用于分类任务。\n",
    "- **特点**:\n",
    "  - 支持多分类问题。\n",
    "  - 可以处理连续型和离散型特征。\n",
    "- **关键参数**:\n",
    "  - `criterion`: 划分标准，默认为 `'gini'`（基尼系数），也可以选择 `'entropy'`（信息增益）。\n",
    "  - `max_depth`: 树的最大深度，用于控制模型复杂度。\n",
    "  - `min_samples_split`: 内部节点再划分所需的最小样本数。\n",
    "  - `min_samples_leaf`: 叶节点所需的最小样本数。\n",
    "  - `random_state`: 随机种子，用于复现结果。\n",
    "\n",
    "- **示例代码**:\n",
    "  ```python\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "  from sklearn.datasets import load_iris\n",
    "  from sklearn.model_selection import train_test_split\n",
    "\n",
    "  # 加载数据集\n",
    "  X, y = load_iris(return_X_y=True)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "  # 构建模型\n",
    "  clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "  clf.fit(X_train, y_train)\n",
    "\n",
    "  # 预测\n",
    "  print(\"测试集准确率:\", clf.score(X_test, y_test))\n",
    "  \n",
    "  ```\n",
    "\n",
    "\n",
    "### 2 `DecisionTreeRegressor`\n",
    "- **用途**: 用于回归任务。\n",
    "- **特点**:\n",
    "  - 适用于连续型目标变量。\n",
    "  - 可以捕捉复杂的非线性关系。\n",
    "- **关键参数**:\n",
    "  - `criterion`: 划分标准,默认为 'mse'（均方误差），也可以选择 'friedman_mse' 或 'mae'。\n",
    "  - 其他参数与 DecisionTreeClassifier 类似。\n",
    "\n",
    "- **示例代码**:\n",
    "  ```python\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "  from sklearn.tree import DecisionTreeRegressor\n",
    "  from sklearn.datasets import load_diabetes\n",
    "  from sklearn.model_selection import train_test_split\n",
    "\n",
    "  # 加载数据集\n",
    "  X, y = load_diabetes(return_X_y=True)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "  # 构建模型\n",
    "  reg = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "  reg.fit(X_train, y_train)\n",
    "\n",
    "  # 预测\n",
    "  print(\"测试集 R^2 得分:\", reg.score(X_test, y_test))\n",
    "  ```\n",
    "\n",
    "\n",
    "  ### 3 `export_text`\n",
    "- **用途**: 将决策树的结构导出为文本格式，便于可视化和解释。\n",
    "- **特点**:\n",
    "  - 提供了树的分裂规则和叶节点的预测值。\n",
    "\n",
    "- **示例代码**:\n",
    "  ```python\n",
    "  from sklearn.tree import export_text\n",
    "\n",
    "  # 导出树结构\n",
    "  tree_rules = export_text(clf, feature_names=load_iris().feature_names)\n",
    "  print(tree_rules)\n",
    "  ```\n",
    "  \n",
    "  \n",
    "  ### 4 `plot_tree`\n",
    "- **用途**: 绘制决策树的图形化表示\n",
    "- **特点**:\n",
    "  - 使用 matplotlib 提供直观的树结构可视化。\n",
    "\n",
    "- **示例代码**:\n",
    "  ```python\n",
    "  from sklearn.tree import plot_tree\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  # 绘制决策树\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plot_tree(clf, feature_names=load_iris().feature_names, class_names=load_iris().target_names, filled=True)\n",
    "  plt.show()\n",
    "  \n",
    "  ```\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304303ef",
   "metadata": {},
   "source": [
    "## 基于决策树的泰塔尼克号生还分类及其可视化（sklearn）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a36478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集准确率：C4.5：0.8792134831460674，CART：0.8848314606741573\n",
      "测试集准确率：C4.5：0.7150837988826816，CART：0.7877094972067039\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#criterion表示分类依据，max_depth表示树的最大深度\n",
    "#entroy生成的是C4.5分类树\n",
    "c45=tree.DecisionTreeClassifier(criterion='entropy',max_depth=6)\n",
    "c45.fit(train_x,train_y)\n",
    "\n",
    "#gini生成的是CART分类树\n",
    "cart=tree.DecisionTreeClassifier(criterion='gini',max_depth=6)\n",
    "cart.fit(train_x,train_y)\n",
    "\n",
    "c45_train_pred=c45.predict(train_x)\n",
    "c45_test_pred=c45.predict(test_x)\n",
    "\n",
    "cart_train_pred=cart.predict(train_x)\n",
    "cart_test_pred=cart.predict(test_x)\n",
    "\n",
    "\n",
    "print(f'训练集准确率：C4.5：{np.mean(c45_train_pred == train_y)}，' \\\n",
    "    f'CART：{np.mean(cart_train_pred == train_y)}')\n",
    "print(f'测试集准确率：C4.5：{np.mean(c45_test_pred == test_y)}，' \\\n",
    "    f'CART：{np.mean(cart_test_pred == test_y)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7851d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#StringIO是Python 中的一个工具，用于在内存中创建一个类似文件的对象。\n",
    "#在这里，StringIO 被用来存储 export_graphviz 生成的 .dot 格式数据，而不是直接写入磁盘文件。 \n",
    "from six import StringIO\n",
    "#pydotplus 是一个 Python 库，用于将 Graphviz 的 .dot 文件转换为图像格式（如 PNG、PDF 等）。\n",
    "#它允许我们将决策树的结构导出为可视化的图像。\n",
    "import pydotplus \n",
    "\n",
    "dot_data=StringIO()  #创建一个内存中的字符串缓冲区，用于存储.dot格式的决策树描述。\n",
    "tree.export_graphviz(  #用于将决策树模型导出为Graphviz的.dot格式\n",
    "    c45,      #决策树模型对象 \n",
    "    out_file=dot_data,  #指定输出的目标文件或对象。这里使用StringIO对象，将.dot 数据存储在内存中。\n",
    "    feature_names=feat_names,  #指定特征名称列表（如['Age', 'Sex', 'Fare']），用于标注树中的分裂节点。\n",
    "    class_names=['no-survival','survival'],  #指定目标变量的类别名称（如 ['non-survival', 'survival']），用于标注叶节点的预测结果。\n",
    "    filled=True,     #使用颜色填充节点，便于区分不同类别的分布。\n",
    "    rounded=True,    #绘制圆角矩形节点，使图像更美观。\n",
    "    impurity=False  #不显示节点的不纯度（如基尼系数或信息熵）。\n",
    ")\n",
    "\n",
    "# 用pydotplus生成图像\n",
    "\n",
    "graph=pydotplus.graph_from_dot_data(\n",
    "    dot_data.getvalue().replace('\\n','')  #去除换行符，确保数据格式正确。\n",
    ")\n",
    "graph.write_png('tree.png')  #打印后的value 表示每个类别的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e71529b",
   "metadata": {},
   "source": [
    "### **CART 树（Classification and Regression Tree）**\n",
    "\n",
    "**CART（分类与回归树）** 是一种经典的决策树算法，由 Breiman 等人在 1984 年提出。它既可以用于**分类任务**（输出离散类别），也可以用于**回归任务**（输出连续值）。CART 的核心思想是通过递归地将数据集划分为子集，构建一棵二叉树，使得每个叶节点尽可能“纯净”或预测误差最小。\n",
    "\n",
    "---\n",
    "\n",
    "## **一、CART 的基本思想**\n",
    "\n",
    "### **1. 二叉树结构**\n",
    "不同于 ID3 和 C4.5 使用多叉树（一个节点分裂为多个分支），**CART 总是生成二叉树**，即每个节点只分裂为两个子节点。这意味着对于一个多值特征，CART 会将其拆解为多个二元划分。\n",
    "\n",
    "例如：\n",
    "- 特征 `天气` 有三个取值：晴天、阴天、雨天。\n",
    "- CART 可能先将数据分为 `天气 == 晴天` vs `天气 != 晴天`，再在后续节点中继续细分。\n",
    "\n",
    "### **2. 分裂标准**\n",
    "CART 使用以下两种不同的标准进行特征选择和分裂：\n",
    "\n",
    "####  **分类任务：基尼指数（Gini Index）**\n",
    "基尼指数衡量的是从数据集中随机选取两个样本，其类别不一致的概率。基尼指数越小，表示数据越“纯净”。\n",
    "\n",
    "公式如下：\n",
    "$$\n",
    "\\text{Gini}(D) = 1 - \\sum_{k=1}^K p_k^2\n",
    "$$\n",
    "其中，$p_k$ 是类别 $k$ 在数据集 $D$ 中的比例。\n",
    "\n",
    "对于某个特征 $A$ 的分裂方式，计算其加权基尼指数：\n",
    "$$\n",
    "\\text{Gini}_A(D) = \\frac{|D_1|}{|D|} \\cdot \\text{Gini}(D_1) + \\frac{|D_2|}{|D|} \\cdot \\text{Gini}(D_2)\n",
    "$$\n",
    "\n",
    "选择使 $\\text{Gini}_A(D)$ 最小的特征和划分方式进行分裂。\n",
    "\n",
    "####  **回归任务：平方误差（Squared Error）**\n",
    "在回归任务中，CART 使用均方误差（MSE）作为分裂标准。目标是最小化预测值与真实值之间的误差。\n",
    "\n",
    "假设当前节点的数据集为 $D$，分裂后的两个子集为 $D_1$ 和 $D_2$，则分裂后的总误差为：\n",
    "$$\n",
    "E = \\sum_{x_i \\in D_1} (y_i - \\bar{y}_1)^2 + \\sum_{x_i \\in D_2} (y_i - \\bar{y}_2)^2\n",
    "$$\n",
    "其中，$\\bar{y}_1$ 和 $\\bar{y}_2$ 是两个子集的目标值的均值。\n",
    "\n",
    "选择使 $E$ 最小的特征和划分点进行分裂。\n",
    "\n",
    "---\n",
    "\n",
    "## **二、CART 构建过程**\n",
    "\n",
    "1. **初始化根节点**：整个训练集作为初始数据集。\n",
    "2. **遍历所有特征和可能的划分点**：\n",
    "   - 对每个特征，尝试不同的划分阈值。\n",
    "   - 计算每个划分对应的基尼指数（分类）或平方误差（回归）。\n",
    "3. **选择最优划分**：选择使基尼指数最小或误差最小的特征和划分点。\n",
    "4. **递归划分子节点**：对每个子集重复上述过程，直到满足停止条件。\n",
    "5. **剪枝优化**：使用代价复杂度剪枝（Cost Complexity Pruning）来防止过拟合。\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **三、示例说明**\n",
    "\n",
    "### **分类任务示例**\n",
    "假设我们有一个数据集，包含 10 个样本，目标变量为是否购买商品（是/否），特征包括年龄（青年、中年、老年）、收入（高、低）等。\n",
    "\n",
    "#### **步骤 1：计算基尼指数**\n",
    "当前节点有 6 个“是”，4 个“否”：\n",
    "$$\n",
    "\\text{Gini}(D) = 1 - (0.6)^2 - (0.4)^2 = 1 - 0.36 - 0.16 = 0.48\n",
    "$$\n",
    "\n",
    "#### **步骤 2：尝试按年龄划分**\n",
    "- 划分 1：青年 vs 非青年\n",
    "  - 青年子集：4 个样本，2 个“是”，2 个“否” → Gini = 0.5\n",
    "  - 非青年子集：6 个样本，4 个“是”，2 个“否” → Gini ≈ 0.444\n",
    "  - 加权 Gini ≈ $0.4 \\times 0.5 + 0.6 \\times 0.444 = 0.466$\n",
    "\n",
    "- 划分 2：中年 vs 非中年\n",
    "  - 中年子集：3 个样本，3 个“是”，0 个“否” → Gini = 0\n",
    "  - 非中年子集：7 个样本，3 个“是”，4 个“否” → Gini ≈ 0.489\n",
    "  - 加权 Gini ≈ $0.3 \\times 0 + 0.7 \\times 0.489 = 0.342$\n",
    "\n",
    "由于划分 2 的加权 Gini 更小，因此选择按“是否中年”进行分裂。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e16cf",
   "metadata": {},
   "source": [
    "# ID3、C4.5 与 CART \n",
    "\n",
    "| 特性                     | **ID3**                           | **C4.5**                                                                 | **CART**                                                                 |\n",
    "|--------------------------|------------------------------------|--------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
    "| 全称                    | Iterative Dichotomiser 3            | C4.5                                                                    | Classification and Regression Tree                                      |\n",
    "| 树形结构                | 多叉树                             | 多叉树                                                                  | 二叉树                                                                  |\n",
    "| 适用任务                | 分类（离散目标）                  | 分类（离散目标）                                                       | 分类和回归（支持连续目标）                                             |\n",
    "| 分裂准则                | 信息增益（Information Gain）      | 信息增益率（Gain Ratio）                                               | 基尼系数（Gini Index，分类）或均方误差（MSE，回归）                   |\n",
    "| 是否支持剪枝             | 否                                 | 是（悲观错误剪枝，PEP）                                                | 是（代价复杂度剪枝，Cost-Complexity Pruning）                           |\n",
    "| 是否处理缺失值           | 否                                 | 是（通过概率分布加权）                                                 | 是（通过替代分裂等策略）                                               |\n",
    "| 是否处理连续特征         | 否                                 | 是（自动进行离散化）                                                   | 是（在训练时自动选择最优切分点）                                       |\n",
    "| 是否需要预剪枝           | 是（需手动控制深度或节点大小）     | 可选（依赖后剪枝）                                                    | 支持（可通过 `max_depth` 等参数控制）                                  |\n",
    "| 叶节点预测方式           | 多数投票                          | 多数投票 / 概率输出                                                    | 分类：多数类别<br>回归：区域内目标值的均值或中位数                      |\n",
    "| 对特征取值较多的偏好     | 有                                | 无（用增益率代替信息增益）                                            | 无（使用基尼指数或 MSE 自动平衡）                                      |\n",
    "| 主要优点                 | 简单直观                          | 支持缺失值与连续特征<br>避免过拟合                                     | 支持分类和回归<br>支持剪枝<br>模型可解释性强                            |\n",
    "| 主要缺点                 | 不支持缺失值与连续特征<br>容易过拟合 | 输出树结构较复杂<br>容易生成深层树导致过拟合                            | 叶子节点预测为局部平均或中位数<br>对噪声数据敏感                        |\n",
    "\n",
    "---\n",
    "\n",
    "### **核心公式对比**\n",
    "\n",
    "#### **ID3 — 信息增益（Information Gain）**\n",
    "$$\n",
    "IG(D, A) = H(D) - H(D|A)\n",
    "$$\n",
    "其中：\n",
    "- $ H(D) $: 数据集 $ D $ 的信息熵。\n",
    "- $ H(D|A) $: 在特征 $ A $ 条件下的条件熵。\n",
    "\n",
    "#### **C4.5 — 信息增益率（Gain Ratio）**\n",
    "$$\n",
    "IGR(D, A) = \\frac{IG(D, A)}{H_A(D)}\n",
    "$$\n",
    "其中：\n",
    "- $ H_A(D) $: 特征 $ A $ 的固有值（Intrinsic Value），用于惩罚取值过多的特征。\n",
    "\n",
    "#### **CART — 分类用基尼系数（Gini Index）**\n",
    "$$\n",
    "Gini(D) = 1 - \\sum_{i=1}^{k} p_i^2\n",
    "$$\n",
    "其中：\n",
    "- $ p_i $: 第 $ i $ 类样本在当前节点中的比例。\n",
    "\n",
    "#### **CART — 回归用均方误差（MSE）**\n",
    "$$\n",
    "MSE(D) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\bar{y})^2\n",
    "$$\n",
    "其中：\n",
    "- $ y_i $: 样本的真实目标值。\n",
    "- $ \\bar{y} $: 当前节点中所有样本目标值的均值。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
