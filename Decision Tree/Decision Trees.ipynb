{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90225ac4",
   "metadata": {},
   "source": [
    "# 决策树算法：ID3 和 C4.5 \n",
    "\n",
    "## 1. 决策树简介\n",
    "\n",
    "决策树是一种基于树形结构的分类和回归模型，广泛应用于机器学习领域。其核心思想是通过一系列规则（特征选择）对数据进行划分，最终形成一个树状结构，用于预测目标变量。\n",
    "\n",
    "- **优点**:\n",
    "  - 直观易理解。\n",
    "  - 能够处理分类和回归问题。\n",
    "  - 对缺失值敏感性较低。\n",
    "\n",
    "- **缺点**:\n",
    "  - 容易过拟合。\n",
    "  - 对噪声数据较为敏感。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ID3 算法\n",
    "\n",
    "### 2.1 基本思想\n",
    "\n",
    "ID3（Iterative Dichotomiser 3）是一种经典的决策树构建算法，基于信息增益选择最优特征进行分裂。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 信息熵（Entropy）\n",
    "\n",
    "信息熵用于衡量随机变量的不确定性。对于一个离散随机变量 $ X $，其取值范围为 $ \\{1, 2, \\dots, k\\} $，信息熵定义为：\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^{k} P(X = i) \\log_2 P(X = i)\n",
    "$$\n",
    "其中：\n",
    "- $ P(X = i) $ 表示随机变量 $ X $ 取值为 $ i $ 的概率。\n",
    "- 当所有样本属于同一类时，信息熵达到最小值 $ H(X) = 0 $。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 条件熵（Conditional Entropy）\n",
    "\n",
    "条件熵表示在已知某个特征 $ A $ 的情况下，数据集的不确定性。假设特征 $ A $ 的取值范围为 $ \\{a_1, a_2, \\dots, a_m\\} $，则条件熵定义为：\n",
    "$$\n",
    "H(D|A) = \\sum_{j=1}^{m} \\frac{|D_j|}{|D|} H(D_j)\n",
    "$$\n",
    "其中：\n",
    "- $ D_j $ 是特征 $ A $ 取值为 $ a_j $ 的子集。\n",
    "- $ |D| $ 和 $ |D_j| $ 分别表示数据集 $ D $ 和子集 $ D_j $ 的样本数量。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 信息增益（Information Gain）\n",
    "\n",
    "信息增益表示使用特征 $ A $ 划分数据集后，信息熵的减少量。其公式为：\n",
    "$$\n",
    "IG(D, A) = H(D) - H(D|A)\n",
    "$$\n",
    "其中：\n",
    "- $ H(D) $ 是数据集 $ D $ 的初始熵。\n",
    "- $ H(D|A) $ 是在特征 $ A $ 的条件下，数据集 $ D $ 的条件熵。\n",
    "\n",
    "信息增益越大，说明使用特征 $ A $ 划分数据集的效果越好。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 算法步骤\n",
    "\n",
    "1. 计算数据集的初始熵 $ H(D) $。\n",
    "2. 遍历所有特征，计算每个特征的信息增益 $ IG(D, A) $。\n",
    "3. 选择信息增益最大的特征作为当前节点的分裂特征。\n",
    "4. 递归地对子集重复上述过程，直到满足停止条件（如所有样本属于同一类或没有更多特征可用）。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. C4.5 算法\n",
    "\n",
    "C4.5 是 ID3 的改进版本，主要解决了以下问题：\n",
    "- 使用信息增益率代替信息增益，避免偏向于取值较多的特征。\n",
    "- 能够处理连续型特征。\n",
    "- 支持缺失值的处理。\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 信息增益率（Gain Ratio）\n",
    "\n",
    "信息增益率是对信息增益的归一化处理，定义为：\n",
    "$$\n",
    "GR(D, A) = \\frac{IG(D, A)}{H_A(D)}\n",
    "$$\n",
    "其中：\n",
    "- $ H_A(D) $ 是特征 $ A $ 的固有值（Intrinsic Value），表示特征 $ A $ 的不确定性：\n",
    "$$\n",
    "H_A(D) = -\\sum_{j=1}^{m} \\frac{|D_j|}{|D|} \\log_2\\left(\\frac{|D_j|}{|D|}\\right)\n",
    "$$\n",
    "\n",
    "信息增益率可以有效避免信息增益对取值较多特征的偏好。\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 连续特征处理\n",
    "\n",
    "对于连续特征，C4.5 通过以下方法处理：\n",
    "1. 将连续特征的取值排序。\n",
    "2. 在相邻取值之间生成候选分割点。\n",
    "3. 计算每个分割点的信息增益率，选择最优分割点。\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 缺失值处理\n",
    "\n",
    "C4.5 提供了两种处理缺失值的方式：\n",
    "1. **训练阶段**:\n",
    "   - 忽略缺失值样本，仅使用非缺失值样本计算信息增益。\n",
    "2. **预测阶段**:\n",
    "   - 如果样本在某特征上缺失，则根据该特征的分布概率分配到不同分支。\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 算法步骤\n",
    "\n",
    "1. 计算数据集的初始熵 $ H(D) $。\n",
    "2. 遍历所有特征，计算每个特征的信息增益率 $ GR(D, A) $。\n",
    "3. 选择信息增益率最大的特征作为当前节点的分裂特征。\n",
    "4. 递归地对子集重复上述过程，直到满足停止条件。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 正则化项的引入\n",
    "\n",
    "为了防止过拟合，C4.5 引入了正则化项来控制模型复杂度。总代价函数定义为：\n",
    "$$\n",
    "\\text{Total Cost} = \\text{Data Complexity} + \\text{Model Complexity}\n",
    "$$\n",
    "其中：\n",
    "- **数据复杂性（Data Complexity）**:\n",
    "  - 衡量分裂后子集的不确定性（熵）。\n",
    "  - 计算方式为：\n",
    "    $$\n",
    "    \\text{Data Complexity} = \\sum_{i=1}^{k} |D_i| \\cdot H(D_i)\n",
    "    $$\n",
    "    其中：\n",
    "    - $ D_i $ 是第 $ i $ 个子集。\n",
    "    - $ H(D_i) $ 是子集的熵。\n",
    "\n",
    "- **模型复杂性（Model Complexity）**:\n",
    "  - 衡量模型的复杂度（叶节点数量）。\n",
    "  - 计算方式为：\n",
    "    $$\n",
    "    \\text{Model Complexity} = \\lambda \\cdot T\n",
    "    $$\n",
    "    其中：\n",
    "    - $ T $ 是叶节点数量。\n",
    "    - $ \\lambda $ 是正则化系数。\n",
    "\n",
    "最终代价函数为：\n",
    "$$\n",
    "\\text{Total Cost} = \\sum_{i=1}^{k} |D_i| \\cdot H(D_i) + \\lambda \\cdot T\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 总结\n",
    "\n",
    "- **ID3**:\n",
    "  - 使用信息增益选择最优特征。\n",
    "  - 存在对取值较多特征的偏好问题。\n",
    "\n",
    "- **C4.5**:\n",
    "  - 使用信息增益率解决 ID3 的偏好问题。\n",
    "  - 支持连续特征和缺失值的处理。\n",
    "\n",
    "决策树算法的核心在于通过信息论的方法评估特征的重要性，并以此构建树形结构进行分类或回归。\n",
    "\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc13d9",
   "metadata": {},
   "source": [
    "#### 本章用到的数据集是泰坦尼克号的生存预测数据集，它包含了许多泰坦尼克号上的乘客的特征信息，以及该乘客最后是否生还。\n",
    "\n",
    "\n",
    "| 列编号 | 特征名称       | 含义                     | 取值                   |\n",
    "|--------|----------------|--------------------------|------------------------|\n",
    "| 0      | 'PassengerId'  | 编号，从1开始            | 整数                   |\n",
    "| 1      | 'Survived'     | 是否生还，1代表生还，0代表遇难 | 0、1                  |\n",
    "| 2      | 'Pclass'       | 舱位等级                 | 0、1、2                |\n",
    "| 3      | 'Name'         | 姓名                     | 字符串                 |\n",
    "| 4      | 'Sex'          | 性别                     | 'male'、'female'       |\n",
    "| 5      | 'Age'          | 年龄                     | 浮点数                 |\n",
    "| 6      | 'SibSp'        | 登船的兄弟姐妹数量       | 整数                   |\n",
    "| 7      | 'Parch'        | 登船的父母和子女数量     | 整数                   |\n",
    "| 8      | 'Ticket'       | 船票编号                 | 字符串                 |\n",
    "| 9      | 'Fare'         | 船票价格                 | 浮点数                 |\n",
    "| 10     | 'Cabin'        | 所在船舱编号             | 字符串                 |\n",
    "| 11     | 'Embarked'     | 登船港口，C代表瑟堡，S代表南安普敦，Q代表昆斯敦 | 'C'、'S'、'Q'         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e2596e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Cabin Embarked\n",
      "0         0       3    male  22.0      1      0   7.2500   NaN        S\n",
      "1         1       1  female  38.0      1      0  71.2833   C85        C\n",
      "2         1       3  female  26.0      0      0   7.9250   NaN        S\n",
      "3         1       1  female  35.0      1      0  53.1000  C123        S\n",
      "4         0       3    male  35.0      0      0   8.0500   NaN        S\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#读取数据\n",
    "data=pd.read_csv('titanic/train.csv')\n",
    "\n",
    "#查看数据信息和前五行具体内容，其中NaN代表数据缺失\n",
    "print(data.info())\n",
    "print(data[:5])\n",
    "\n",
    "#删去编号\n",
    "#columns=['PassengerId', 'Name', 'Ticket']: 指定要删除的列名。\n",
    "#inplace=True: 表示直接在原 DataFrame 上修改，而不是返回一个新的 DataFrame。\n",
    "data.drop(columns=['PassengerId','Name','Ticket'],inplace=True)\n",
    "\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23809f5e",
   "metadata": {},
   "source": [
    "在上面的算法介绍中，我们只考虑了特征取离散值的情况。在实践中，还有许多特征的取值是连续的，例如本数据集中的年龄和船票价格两项。对于这样的特征，我们可以根据数据的范围划出几个分类点 $x_1, \\dots, x_K$，并按照取值与分类点的大小关系进行分类。具体来说，可以将数据分成以下 $K+1$ 类：\n",
    "\n",
    "- $(-\\infty, x_1]$\n",
    "- $(x_1, x_2]$\n",
    "- $\\dots$\n",
    "- $(x_K, +\\infty)$\n",
    "\n",
    "这样，我们就把连续的数据转化成了离散的取值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1409b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age ：\n",
      "0.4200\n",
      "9.2622\n",
      "18.1044\n",
      "26.9467\n",
      "35.7889\n",
      "44.6311\n",
      "53.4733\n",
      "62.3156\n",
      "71.1578\n",
      "80.0000\n",
      "Fare ：\n",
      "0.0000\n",
      "56.9255\n",
      "113.8509\n",
      "170.7764\n",
      "227.7019\n",
      "284.6273\n",
      "341.5528\n",
      "398.4783\n",
      "455.4037\n",
      "512.3292\n"
     ]
    }
   ],
   "source": [
    "feat_ranges = {} #字典\n",
    "cont_feat = ['Age', 'Fare'] # 连续特征\n",
    "bins = 10 # 分类点数\n",
    "\n",
    "for feat in cont_feat:\n",
    "    # 数据集中存在缺省值nan，需要用np.nanmin和np.nanmax,如果直接使用 min() 或 max()，当列中存在NaN时会抛出错误或返回 NaN\n",
    "    # 计算当前特征列的最小值和最大值，忽略缺失值（NaN）。\n",
    "    min_val = np.nanmin(data[feat])   \n",
    "    max_val = np.nanmax(data[feat])   \n",
    "    feat_ranges[feat] = np.linspace(min_val, max_val, bins).tolist()  #.tolist()将NumPy数组转换为Python列表，方便后续操作。\n",
    "    print(feat, '：') # 查看分类点\n",
    "    for spt in feat_ranges[feat]:\n",
    "        print(f'{spt:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e6acc",
   "metadata": {},
   "source": [
    "#### 介绍一下分类类型\n",
    "\n",
    "分类类型是指数据的取值范围是有限的、离散的集合，而不是连续的数值。例如：\n",
    "\n",
    "- 性别 ：`['male', 'female']`\n",
    "- 颜色 ：`['red', 'green', 'blue']` \n",
    "\n",
    "\n",
    "分类类型的常见操作\n",
    "\n",
    " - 数据类型转换\n",
    "   \n",
    "   - 在 Pandas 中，可以将分类数据转换为 category 类型\n",
    "\n",
    "    ``` python\n",
    "    data['Sex'] = data['Sex'].astype('category')\n",
    "    ```\n",
    " - 查看类别\n",
    "\n",
    "   - 使用 `cat.categories` 查看所有类别：\n",
    "\n",
    "   ``` python \n",
    "   print(data['Sex'].cat.categories) \n",
    "   #输出示例\n",
    "   # Index(['female', 'male'], dtype='object')\n",
    "   ```\n",
    " \n",
    " - 编码\n",
    "   \n",
    "   - 整数编码（Label Encoding）\n",
    "     \n",
    "     - 将类别按顺序映射为整数：\n",
    "     ``` python\n",
    "     data['Sex'] = data['Sex'].cat.codes\n",
    "     #输出示例\n",
    "     #female -> 0 male -> 1\n",
    "     ```\n",
    "   \n",
    "   - 独热编码（One-Hot Encoding）\n",
    "\n",
    "    - 使用 Pandas 的 get_dummies 方法：\n",
    "     ``` python\n",
    "     data = pd.get_dummies(data, columns=['Sex'])\n",
    "     #输出示例\n",
    "     #   Sex_female  Sex_male\n",
    "     #0      0         1\n",
    "     #1      1         0\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3569e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex:Index(['female', 'male'], dtype='object')\n",
      "Pclass:Index([1, 2, 3], dtype='int64')\n",
      "SibSp:Index([0, 1, 2, 3, 4, 5, 8], dtype='int64')\n",
      "Parch:Index([0, 1, 2, 3, 4, 5, 6], dtype='int64')\n",
      "Cabin:Index(['A10', 'A14', 'A16', 'A19', 'A20', 'A23', 'A24', 'A26', 'A31', 'A32',\n",
      "       ...\n",
      "       'E8', 'F E69', 'F G63', 'F G73', 'F2', 'F33', 'F38', 'F4', 'G6', 'T'],\n",
      "      dtype='object', length=147)\n",
      "Embarked:Index(['C', 'Q', 'S'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 只有有限取值的离散特征\n",
    "cat_feat = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Cabin', 'Embarked'] \n",
    "\n",
    "for feat in cat_feat:\n",
    "    data[feat]=data[feat].astype('category') # 将特征列的数据类型转换为Pandas的category类型\n",
    "    print(f'{feat}:{data[feat].cat.categories}') #查看类别\n",
    "    data[feat]=data[feat].cat.codes.to_list() #将类别按顺序转化为整数\n",
    "    ranges=list(set(data[feat]))  #使用 set(data[feat]) 获取当前特征的所有唯一取值。\n",
    "    ranges.sort()             \n",
    "    feat_ranges[feat]=ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b16fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将所有缺省值替换为-1\n",
    "data.fillna(-1,inplace=True)  \n",
    "for feat in feat_ranges.keys():  # 遍历字典feat_ranges中的所有键（即特征名）。\n",
    "    feat_ranges[feat]=[-1]+feat_ranges[feat] #使用列表拼接操作[-1] + feat_ranges[feat]，确保 -1 成为第一个元素。\n",
    "\n",
    "#这行代码的作用是将 -1 添加到每个特征的取值范围中，确保缺失值的填充值也被视为合法的类别之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f4d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked'], dtype='object')\n",
      "Survived\n",
      "训练集大小: 712\n",
      "测试集大小: 179\n",
      "特征数大小: 8\n"
     ]
    }
   ],
   "source": [
    "#划分数据集与测试集\n",
    "np.random.seed(0)\n",
    "\n",
    "feat_names=data.columns[1:]\n",
    "label_name=data.columns[0]\n",
    "print(feat_names)\n",
    "print(label_name)\n",
    "\n",
    "#重排下标\n",
    "data=data.reindex(np.random.permutation(data.index))\n",
    "ratio=0.8\n",
    "split=int(ratio*len(data))\n",
    "\n",
    "train_x=data[:split].drop(columns='Survived').to_numpy()\n",
    "train_y=data['Survived'][:split].to_numpy()\n",
    "test_x=data[split:].drop(columns='Survived').to_numpy()\n",
    "test_y=data['Survived'][split:].to_numpy()\n",
    "\n",
    "print('训练集大小:',len(train_x))\n",
    "print('测试集大小:',len(test_x))\n",
    "print('特征数大小:',train_x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d85b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        #内部结点的feat表示用来分类的特征编号，其数字与数据中的顺序对应\n",
    "        #叶节点的feat表示该结点的分类结果\n",
    "        self.feat=None\n",
    "        #分类值列表，表示按照其中的值向子结点分类\n",
    "        self.split=None\n",
    "        #子节点列表，叶节点的child为空\n",
    "        self.child=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1346e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self,X,Y,feat_ranges,lbd):\n",
    "        self.root=Node()\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "        self.feat_ranges=feat_ranges #特征取值范围\n",
    "        self.lbd=lbd  #正则化系数\n",
    "        self.eps=1e-8 #防止log0的情况\n",
    "        self.T=0  #记录叶节点数\n",
    "        self.ID3(self.root,self.X,self.Y)\n",
    "\n",
    "    #工具函数，计算 a*log a\n",
    "    def aloga(self,a):\n",
    "        return a*np.log2(a+self.eps)\n",
    "    \n",
    "    #计算某个子数据集的熵\n",
    "    def entropy(self,Y):\n",
    "        #统计每个类别出现的次数\n",
    "        #返回的是一个包含两个元素的元组：\n",
    "        # 1 唯一值  2 每个唯一值对应的出现次数。\n",
    "        cnt=np.unique(Y,return_counts=True)[1]   #return_counts=True: 表示返回每个唯一值的出现次数。 \n",
    "        N=len(Y)\n",
    "        ent=-np.sum([self.aloga(Ni/N) for Ni in cnt])\n",
    "        return ent\n",
    "    #计算feat<=val划分数据集的信息增益\n",
    "\n",
    "    def info_gain(self,X,Y,feat,val):\n",
    "        #划分前的熵\n",
    "        N=len(Y)\n",
    "        if N==0 :\n",
    "            return 0\n",
    "        HX=self.entropy(Y)\n",
    "        HXY=0 #H（X|Y）\n",
    "        #分别计算H（X|X_F<=val）和分别计算H（X|X_F>val）\n",
    "        Y_l=Y[X[:,feat]<=val]\n",
    "        HXY+=len(Y_l)/len(Y)*self.entropy(Y_l)\n",
    "        Y_r=Y[X[:,feat]>val]\n",
    "        HXY+=len(Y_r)/len(Y)*self.entropy(Y_r)\n",
    "        return HX-HXY\n",
    "    \n",
    "    #计算特征feat<=val本身的复杂度H_Y(X)\n",
    "    def entropy_YX(self,X,Y,feat,val):\n",
    "        HYX = 0\n",
    "        N = len(Y)\n",
    "        if N == 0:\n",
    "            return 0\n",
    "        Y_l = Y[X[:, feat] <= val]\n",
    "        HYX += -self.aloga(len(Y_l) / N)\n",
    "        Y_r = Y[X[:, feat] > val]\n",
    "        HYX += -self.aloga(len(Y_r) / N)\n",
    "        return HYX\n",
    "    \n",
    "    #计算用feat<=val划分数据集的信息增益率\n",
    "    def info_gain_ratio(self,X,Y,feat,val):\n",
    "        IG=self.info_gain(X,Y,feat,val)\n",
    "        HYX=self.entropy_YX(X,Y,feat,val)\n",
    "        return IG/HYX\n",
    "        \n",
    "    #用ID3算法递归分裂结点，构造决策树\n",
    "    def ID3(self,node,X,Y):\n",
    "        #判断是否已经分类完成\n",
    "        if len(np.unique(Y))==1:\n",
    "            node.feat=Y[0]\n",
    "            self.T+=1\n",
    "            return \n",
    "        \n",
    "        #寻找最优分类特征和分类点\n",
    "        best_IGR=0\n",
    "        best_feat=None\n",
    "        best_val=None\n",
    "        for feat in range(len(feat_names)):\n",
    "            for val in self.feat_ranges[feat_names[feat]]:\n",
    "                IGR=self.info_gain_ratio(X,Y,feat,val)\n",
    "                if IGR>best_IGR:\n",
    "                    best_IGR=IGR\n",
    "                    best_feat=feat\n",
    "                    best_val=val\n",
    "        #计算用best_feat<=best_val分类带来的代价函数变化\n",
    "        #由于分裂叶结点只涉及该局部，我们只需要计算分裂前后该结点的代价函数\n",
    "        #当前代价\n",
    "        cur_cost=len(Y)*self.entropy(Y)+self.lbd\n",
    "        #分裂后的代价，按best_feat的取值分裂统计\n",
    "        #如果best_feat为None  说明(1)数据集中所有样本已经属于同一类别。(2)信息增益率为 0，无法通过分裂进一步减少不确定性。\n",
    "        \n",
    "        #在分类也无法增加信息了，因此将new_cost设置为无穷大表示在这种情况下不进行分裂\n",
    "        if best_feat is None:\n",
    "            new_cost=np.inf\n",
    "        else:\n",
    "            new_cost=0\n",
    "            X_feat=X[:,best_feat]\n",
    "            #获取划分后的两部分，计算新的熵\n",
    "            new_Y_l=Y[X_feat<=best_val]\n",
    "            new_cost+=len(new_Y_l)*self.entropy(new_Y_l)\n",
    "            new_Y_r=Y[X_feat>best_val]\n",
    "            new_cost+=len(new_Y_r)*self.entropy(new_Y_r)\n",
    "            #分裂后会有两个叶节点   【注意】lbd表示对每个叶节点的惩罚力度,用途是为什么减少节点数\n",
    "            new_cost+=2*self.lbd\n",
    "        \n",
    "        if new_cost <= cur_cost:\n",
    "            # 如果分裂后代价更小，那么执行分裂\n",
    "            node.feat = best_feat\n",
    "            node.split = best_val\n",
    "            l_child = Node()\n",
    "            l_X = X[X_feat <= best_val]\n",
    "            l_Y = Y[X_feat <= best_val]\n",
    "            self.ID3(l_child, l_X, l_Y)\n",
    "            r_child = Node()\n",
    "            r_X = X[X_feat > best_val]\n",
    "            r_Y = Y[X_feat > best_val]\n",
    "            self.ID3(r_child, r_X, r_Y)\n",
    "            node.child = [l_child, r_child]\n",
    "        else:\n",
    "            # 否则将当前结点上最多的类别作为该结点的类别\n",
    "            vals, cnt = np.unique(Y, return_counts=True)\n",
    "            node.feat = vals[np.argmax(cnt)]\n",
    "            self.T += 1\n",
    "        \n",
    "    def predict(self,x):\n",
    "        node=self.root\n",
    "        #从根结点开始向下寻找，找到叶结点结束\n",
    "        while node.split is not None:\n",
    "            if x[node.feat] <= node.split:\n",
    "                node=node.child[0]\n",
    "            else:\n",
    "                node=node.child[1]\n",
    "        \n",
    "        #到达叶结点，返回类型\n",
    "        return node.feat\n",
    "    \n",
    "    #计算在样本X，标签Y上的准确率\n",
    "    def accuracy(self,X,Y):\n",
    "        correct=0\n",
    "        for x,y in zip(X,Y):\n",
    "            pred=self.predict(x)\n",
    "            if pred==y:\n",
    "                correct+=1\n",
    "        return correct/len(Y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28fef007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "叶结点数量： 23\n",
      "训练集准确率： 0.8300561797752809\n",
      "测试集准确率： 0.7262569832402235\n"
     ]
    }
   ],
   "source": [
    "DT = DecisionTree(train_x, train_y, feat_ranges, lbd=1.0)\n",
    "print('叶结点数量：', DT.T)\n",
    "\n",
    "# 计算在训练集和测试集上的准确率\n",
    "print('训练集准确率：', DT.accuracy(train_x, train_y))\n",
    "print('测试集准确率：', DT.accuracy(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aedfd5",
   "metadata": {},
   "source": [
    "## Scikit-learn 中的 Tree 模型\n",
    "\n",
    "Scikit-learn（简称 `sklearn`）是 Python 中一个功能强大的机器学习库，提供了多种机器学习算法的实现。其中，`tree` 模块专注于基于决策树的模型，包括分类和回归任务。本文将详细介绍 `sklearn.tree` 的核心内容及其使用方法。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. `sklearn.tree` 模块的核心类\n",
    "\n",
    "`sklearn.tree` 提供了以下主要类来构建和使用决策树模型：\n",
    "\n",
    "### 1.1 `DecisionTreeClassifier`\n",
    "- **用途**: 用于分类任务。\n",
    "- **特点**:\n",
    "  - 支持多分类问题。\n",
    "  - 可以处理连续型和离散型特征。\n",
    "- **关键参数**:\n",
    "  - `criterion`: 划分标准，默认为 `'gini'`（基尼系数），也可以选择 `'entropy'`（信息增益）。\n",
    "  - `max_depth`: 树的最大深度，用于控制模型复杂度。\n",
    "  - `min_samples_split`: 内部节点再划分所需的最小样本数。\n",
    "  - `min_samples_leaf`: 叶节点所需的最小样本数。\n",
    "  - `random_state`: 随机种子，用于复现结果。\n",
    "\n",
    "- **示例代码**:\n",
    "  ```python\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "  from sklearn.datasets import load_iris\n",
    "  from sklearn.model_selection import train_test_split\n",
    "\n",
    "  # 加载数据集\n",
    "  X, y = load_iris(return_X_y=True)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "  # 构建模型\n",
    "  clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "  clf.fit(X_train, y_train)\n",
    "\n",
    "  # 预测\n",
    "  print(\"测试集准确率:\", clf.score(X_test, y_test))\n",
    "  \n",
    "  ```\n",
    "\n",
    "\n",
    "### 2.1 `DecisionTreeRegressor`\n",
    "- **用途**: 用于回归任务。\n",
    "- **特点**:\n",
    "  - 适用于连续型目标变量。\n",
    "  - 可以捕捉复杂的非线性关系。\n",
    "- **关键参数**:\n",
    "  - `criterion`: 划分标准,默认为 'mse'（均方误差），也可以选择 'friedman_mse' 或 'mae'。\n",
    "  - 其他参数与 DecisionTreeClassifier 类似。\n",
    "\n",
    "- **示例代码**:\n",
    "  ```python\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "  from sklearn.tree import DecisionTreeRegressor\n",
    "  from sklearn.datasets import load_diabetes\n",
    "  from sklearn.model_selection import train_test_split\n",
    "\n",
    "  # 加载数据集\n",
    "  X, y = load_diabetes(return_X_y=True)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "  # 构建模型\n",
    "  reg = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "  reg.fit(X_train, y_train)\n",
    "\n",
    "  # 预测\n",
    "  print(\"测试集 R^2 得分:\", reg.score(X_test, y_test))\n",
    "\n",
    "  ```\n",
    "\n",
    "  ### 2.1 `DecisionTreeRegressor`\n",
    "- **用途**: 用于回归任务。\n",
    "- **特点**:\n",
    "  - 适用于连续型目标变量。\n",
    "  - 可以捕捉复杂的非线性关系。\n",
    "- **关键参数**:\n",
    "  - `criterion`: 划分标准,默认为 'mse'（均方误差），也可以选择 'friedman_mse' 或 'mae'。\n",
    "  - 其他参数与 DecisionTreeClassifier 类似。\n",
    "\n",
    "- **示例代码**:\n",
    "  ```python\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "  from sklearn.tree import DecisionTreeRegressor\n",
    "  from sklearn.datasets import load_diabetes\n",
    "  from sklearn.model_selection import train_test_split\n",
    "\n",
    "  # 加载数据集\n",
    "  X, y = load_diabetes(return_X_y=True)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "  # 构建模型\n",
    "  reg = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "  reg.fit(X_train, y_train)\n",
    "\n",
    "  # 预测\n",
    "  print(\"测试集 R^2 得分:\", reg.score(X_test, y_test))\n",
    "\n",
    "  ```\n",
    "\n",
    "\n",
    "  ### 2.3 `export_text`\n",
    "- **用途**: 将决策树的结构导出为文本格式，便于可视化和解释。\n",
    "- **特点**:\n",
    "  - 提供了树的分裂规则和叶节点的预测值。\n",
    "\n",
    "- **示例代码**:\n",
    "  ```python\n",
    "  from sklearn.tree import export_text\n",
    "\n",
    "  # 导出树结构\n",
    "  tree_rules = export_text(clf, feature_names=load_iris().feature_names)\n",
    "  print(tree_rules)\n",
    "  ```\n",
    "  ### 2.3 `plot_tree`\n",
    "- **用途**: 绘制决策树的图形化表示\n",
    "- **特点**:\n",
    "  - 使用 matplotlib 提供直观的树结构可视化。\n",
    "\n",
    "- **示例代码**:\n",
    "  ```python\n",
    "  from sklearn.tree import plot_tree\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  # 绘制决策树\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plot_tree(clf, feature_names=load_iris().feature_names, class_names=load_iris().target_names, filled=True)\n",
    "  plt.show()\n",
    "  \n",
    "  ```\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a36478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集准确率：C4.5：0.8792134831460674，CART：0.8848314606741573\n",
      "测试集准确率：C4.5：0.7150837988826816，CART：0.7877094972067039\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#criterion表示分类依据，max_depth表示树的最大深度\n",
    "#entroy生成的是C4.5分类树\n",
    "c45=tree.DecisionTreeClassifier(criterion='entropy',max_depth=6)\n",
    "c45.fit(train_x,train_y)\n",
    "\n",
    "#gini生成的是CART分类树\n",
    "cart=tree.DecisionTreeClassifier(criterion='gini',max_depth=6)\n",
    "cart.fit(train_x,train_y)\n",
    "\n",
    "c45_train_pred=c45.predict(train_x)\n",
    "c45_test_pred=c45.predict(test_x)\n",
    "\n",
    "cart_train_pred=cart.predict(train_x)\n",
    "cart_test_pred=cart.predict(test_x)\n",
    "\n",
    "\n",
    "print(f'训练集准确率：C4.5：{np.mean(c45_train_pred == train_y)}，' \\\n",
    "    f'CART：{np.mean(cart_train_pred == train_y)}')\n",
    "print(f'测试集准确率：C4.5：{np.mean(c45_test_pred == test_y)}，' \\\n",
    "    f'CART：{np.mean(cart_test_pred == test_y)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7851d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#StringIO是Python 中的一个工具，用于在内存中创建一个类似文件的对象。\n",
    "#在这里，StringIO 被用来存储 export_graphviz 生成的 .dot 格式数据，而不是直接写入磁盘文件。 \n",
    "from six import StringIO\n",
    "#pydotplus 是一个 Python 库，用于将 Graphviz 的 .dot 文件转换为图像格式（如 PNG、PDF 等）。\n",
    "#它允许我们将决策树的结构导出为可视化的图像。\n",
    "import pydotplus \n",
    "\n",
    "dot_data=StringIO()  #创建一个内存中的字符串缓冲区，用于存储.dot格式的决策树描述。\n",
    "tree.export_graphviz(  #用于将决策树模型导出为Graphviz的.dot格式\n",
    "    c45,      #决策树模型对象 \n",
    "    out_file=dot_data,  #指定输出的目标文件或对象。这里使用StringIO对象，将.dot 数据存储在内存中。\n",
    "    feature_names=feat_names,  #指定特征名称列表（如['Age', 'Sex', 'Fare']），用于标注树中的分裂节点。\n",
    "    class_names=['no-survival','survival'],  #指定目标变量的类别名称（如 ['non-survival', 'survival']），用于标注叶节点的预测结果。\n",
    "    filled=True,     #使用颜色填充节点，便于区分不同类别的分布。\n",
    "    rounded=True,    #绘制圆角矩形节点，使图像更美观。\n",
    "    impurity=False  #不显示节点的不纯度（如基尼系数或信息熵）。\n",
    ")\n",
    "\n",
    "# 用pydotplus生成图像\n",
    "\n",
    "graph=pydotplus.graph_from_dot_data(\n",
    "    dot_data.getvalue().replace('\\n','')  #去除换行符，确保数据格式正确。\n",
    ")\n",
    "graph.write_png('tree.png')  #打印后的value 表示每个类别的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e71529b",
   "metadata": {},
   "source": [
    "# CART 回归树\n",
    "\n",
    "## 一、CART 回归树简介\n",
    "\n",
    "CART（Classification and Regression Tree）是一种经典的决策树算法，由 Breiman 筈다\n",
    "\n",
    "### **1. 目标**\n",
    "- 解决 **回归问题**，即预测目标变量为 **连续值** 的场景。\n",
    "- 示例：房价预测、销量预测、温度预测等。\n",
    "\n",
    "### **2. 核心思想**\n",
    "- 使用二叉树结构递归地将输入空间划分为多个子区域。\n",
    "- 每个叶节点输出对应区域中样本目标值的均值或中位数。\n",
    "- 分裂准则基于误差最小化，通常使用 **均方误差（MSE）** 或 **平均绝对误差（MAE）**。\n",
    "\n",
    "---\n",
    "\n",
    "## 二、模型表达式\n",
    "\n",
    "最终的 CART 回归树模型是一个分段常数函数：\n",
    "$$\n",
    "f(x) = \\sum_{t=1}^{T} c_t \\cdot \\mathbb{I}(x \\in R_t)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ T $: 叶节点数量。\n",
    "- $ R_t $: 第 $ t $ 个叶节点对应的特征子空间。\n",
    "- $ \\mathbb{I}(x \\in R_t) $: 指示函数，当样本 $ x $ 属于 $ R_t $ 时为 1，否则为 0。\n",
    "- $ c_t $: 第 $ t $ 个叶节点的预测值，通常是该区域内所有样本目标值的均值：\n",
    "  $$\n",
    "  c_t = \\frac{1}{|R_t|} \\sum_{i: x_i \\in R_t} y_i\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 三、构建过程与分裂准则\n",
    "\n",
    "### **1. 初始根节点**\n",
    "- 整个训练集作为根节点的数据。\n",
    "- 计算初始均方误差（MSE）。\n",
    "\n",
    "### **2. 寻找最优分裂点**\n",
    "\n",
    "#### 对每个特征 $ j $ 和其可能的取值 $ s $，尝试划分数据：\n",
    "- 左子集：\n",
    "  $$\n",
    "  D_L = \\{ (x_i, y_i) \\mid x_{ij} \\leq s \\}\n",
    "  $$\n",
    "- 右子集：\n",
    "  $$\n",
    "  D_R = \\{ (x_i, y_i) \\mid x_{ij} > s \\}\n",
    "  $$\n",
    "\n",
    "#### 分裂后的误差定义为加权均方误差：\n",
    "$$\n",
    "Q(j, s) = \\frac{|D_L|}{|D|} \\cdot MSE(D_L) + \\frac{|D_R|}{|D|} \\cdot MSE(D_R)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$$\n",
    "MSE(D_L) = \\frac{1}{|D_L|} \\sum_{(x_i, y_i) \\in D_L} (y_i - \\bar{y}_L)^2 \\\\\n",
    "MSE(D_R) = \\frac{1}{|D_R|} \\sum_{(x_i, y_i) \\in D_R} (y_i - \\bar{y}_R)^2\n",
    "$$\n",
    "\n",
    "> 💡 在每一步中选择使 $ Q(j,s) $ 最小的特征 $ j^* $ 和划分点 $ s^* $ 进行分裂。\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 递归构建子树**\n",
    "- 将左子集和右子集分别作为新的节点继续递归。\n",
    "- 停止条件包括：\n",
    "  - 达到预设的最大深度。\n",
    "  - 节点中样本数小于设定阈值。\n",
    "  - 当前节点的误差已经足够小。\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 叶节点生成**\n",
    "- 当停止分裂后，创建叶节点。\n",
    "- 叶节点的预测值 $ c_t $ 为该区域内所有样本的目标值的均值：\n",
    "  $$\n",
    "  c_t = \\frac{1}{|R_t|} \\sum_{x_i \\in R_t} y_i\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 四、代价函数与剪枝策略\n",
    "\n",
    "为了防止过拟合，CART 引入了正则化项。定义代价复杂度函数为：\n",
    "$$\n",
    "C_\\alpha(T) = C(T) + \\alpha \\cdot T\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ C(T) $: 所有叶节点的误差总和。\n",
    "- $ T $: 叶节点的数量。\n",
    "- $ \\alpha $: 正则化参数，控制模型复杂度。\n",
    "\n",
    "### **剪枝过程**\n",
    "1. 先构建一棵完全生长的树。\n",
    "2. 从叶节点向上回溯，计算每个节点剪枝前后对代价函数的影响。\n",
    "3. 如果剪枝后 $ C_\\alpha(T) $ 更小，则执行剪枝。\n",
    "\n",
    "---\n",
    "\n",
    "## 五、CART 回归树 vs 分类树对比\n",
    "\n",
    "| 特征         | 分类树（如 ID3 / C4.5）        | 回归树（CART）               |\n",
    "|--------------|-------------------------------|-------------------------------|\n",
    "| 目标变量     | 离散类别                     | 连续数值                      |\n",
    "| 分裂准则     | 信息增益、增益率              | 均方误差（MSE）              |\n",
    "| 叶节点预测   | 多数投票                      | 区域内目标值的均值            |\n",
    "| 剪枝方式     | 后剪枝                       | 代价复杂度剪枝                 |\n",
    "\n",
    "---\n",
    "\n",
    "## 六、Python 示例代码\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 示例数据\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = np.sin(X[:, 0]) + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "# 数据集划分\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# 构建回归树\n",
    "reg = DecisionTreeRegressor(max_depth=3)\n",
    "reg.fit(train_x, train_y)\n",
    "\n",
    "# 频繁调整 max_depth 控制树的复杂度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e16cf",
   "metadata": {},
   "source": [
    "# ID3、C4.5 与 CART \n",
    "\n",
    "| 特性                     | **ID3**                           | **C4.5**                                                                 | **CART**                                                                 |\n",
    "|--------------------------|------------------------------------|--------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
    "| 全称                    | Iterative Dichotomiser 3            | C4.5                                                                    | Classification and Regression Tree                                      |\n",
    "| 树形结构                | 多叉树                             | 多叉树                                                                  | 二叉树                                                                  |\n",
    "| 适用任务                | 分类（离散目标）                  | 分类（离散目标）                                                       | 分类和回归（支持连续目标）                                             |\n",
    "| 分裂准则                | 信息增益（Information Gain）      | 信息增益率（Gain Ratio）                                               | 基尼系数（Gini Index，分类）或均方误差（MSE，回归）                   |\n",
    "| 是否支持剪枝             | 否                                 | 是（悲观错误剪枝，PEP）                                                | 是（代价复杂度剪枝，Cost-Complexity Pruning）                           |\n",
    "| 是否处理缺失值           | 否                                 | 是（通过概率分布加权）                                                 | 是（通过替代分裂等策略）                                               |\n",
    "| 是否处理连续特征         | 否                                 | 是（自动进行离散化）                                                   | 是（在训练时自动选择最优切分点）                                       |\n",
    "| 是否需要预剪枝           | 是（需手动控制深度或节点大小）     | 可选（依赖后剪枝）                                                    | 支持（可通过 `max_depth` 等参数控制）                                  |\n",
    "| 叶节点预测方式           | 多数投票                          | 多数投票 / 概率输出                                                    | 分类：多数类别<br>回归：区域内目标值的均值或中位数                      |\n",
    "| 对特征取值较多的偏好     | 有                                | 无（用增益率代替信息增益）                                            | 无（使用基尼指数或 MSE 自动平衡）                                      |\n",
    "| 主要优点                 | 简单直观                          | 支持缺失值与连续特征<br>避免过拟合                                     | 支持分类和回归<br>支持剪枝<br>模型可解释性强                            |\n",
    "| 主要缺点                 | 不支持缺失值与连续特征<br>容易过拟合 | 输出树结构较复杂<br>容易生成深层树导致过拟合                            | 叶子节点预测为局部平均或中位数<br>对噪声数据敏感                        |\n",
    "\n",
    "---\n",
    "\n",
    "### **核心公式对比**\n",
    "\n",
    "#### **ID3 — 信息增益（Information Gain）**\n",
    "$$\n",
    "IG(D, A) = H(D) - H(D|A)\n",
    "$$\n",
    "其中：\n",
    "- $ H(D) $: 数据集 $ D $ 的信息熵。\n",
    "- $ H(D|A) $: 在特征 $ A $ 条件下的条件熵。\n",
    "\n",
    "#### **C4.5 — 信息增益率（Gain Ratio）**\n",
    "$$\n",
    "GR(D, A) = \\frac{IG(D, A)}{H_A(D)}\n",
    "$$\n",
    "其中：\n",
    "- $ H_A(D) $: 特征 $ A $ 的固有值（Intrinsic Value），用于惩罚取值过多的特征。\n",
    "\n",
    "#### **CART — 分类用基尼系数（Gini Index）**\n",
    "$$\n",
    "Gini(D) = 1 - \\sum_{i=1}^{k} p_i^2\n",
    "$$\n",
    "其中：\n",
    "- $ p_i $: 第 $ i $ 类样本在当前节点中的比例。\n",
    "\n",
    "#### **CART — 回归用均方误差（MSE）**\n",
    "$$\n",
    "MSE(D) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\bar{y})^2\n",
    "$$\n",
    "其中：\n",
    "- $ y_i $: 样本的真实目标值。\n",
    "- $ \\bar{y} $: 当前节点中所有样本目标值的均值。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
