{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4bc13d9",
   "metadata": {},
   "source": [
    "#### 本章用到的数据集是泰坦尼克号的生存预测数据集，它包含了许多泰坦尼克号上的乘客的特征信息，以及该乘客最后是否生还。\n",
    "\n",
    "\n",
    "| 列编号 | 特征名称       | 含义                     | 取值                   |\n",
    "|--------|----------------|--------------------------|------------------------|\n",
    "| 0      | 'PassengerId'  | 编号，从1开始            | 整数                   |\n",
    "| 1      | 'Survived'     | 是否生还，1代表生还，0代表遇难 | 0、1                  |\n",
    "| 2      | 'Pclass'       | 舱位等级                 | 0、1、2                |\n",
    "| 3      | 'Name'         | 姓名                     | 字符串                 |\n",
    "| 4      | 'Sex'          | 性别                     | 'male'、'female'       |\n",
    "| 5      | 'Age'          | 年龄                     | 浮点数                 |\n",
    "| 6      | 'SibSp'        | 登船的兄弟姐妹数量       | 整数                   |\n",
    "| 7      | 'Parch'        | 登船的父母和子女数量     | 整数                   |\n",
    "| 8      | 'Ticket'       | 船票编号                 | 字符串                 |\n",
    "| 9      | 'Fare'         | 船票价格                 | 浮点数                 |\n",
    "| 10     | 'Cabin'        | 所在船舱编号             | 字符串                 |\n",
    "| 11     | 'Embarked'     | 登船港口，C代表瑟堡，S代表南安普敦，Q代表昆斯敦 | 'C'、'S'、'Q'         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7e2596e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Cabin Embarked\n",
      "0         0       3    male  22.0      1      0   7.2500   NaN        S\n",
      "1         1       1  female  38.0      1      0  71.2833   C85        C\n",
      "2         1       3  female  26.0      0      0   7.9250   NaN        S\n",
      "3         1       1  female  35.0      1      0  53.1000  C123        S\n",
      "4         0       3    male  35.0      0      0   8.0500   NaN        S\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#读取数据\n",
    "data=pd.read_csv('titanic/train.csv')\n",
    "\n",
    "#查看数据信息和前五行具体内容，其中NaN代表数据缺失\n",
    "print(data.info())\n",
    "print(data[:5])\n",
    "\n",
    "#删去编号\n",
    "#columns=['PassengerId', 'Name', 'Ticket']: 指定要删除的列名。\n",
    "#inplace=True: 表示直接在原 DataFrame 上修改，而不是返回一个新的 DataFrame。\n",
    "data.drop(columns=['PassengerId','Name','Ticket'],inplace=True)\n",
    "\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23809f5e",
   "metadata": {},
   "source": [
    "在上面的算法介绍中，我们只考虑了特征取离散值的情况。在实践中，还有许多特征的取值是连续的，例如本数据集中的年龄和船票价格两项。对于这样的特征，我们可以根据数据的范围划出几个分类点 $x_1, \\dots, x_K$，并按照取值与分类点的大小关系进行分类。具体来说，可以将数据分成以下 $K+1$ 类：\n",
    "\n",
    "- $(-\\infty, x_1]$\n",
    "- $(x_1, x_2]$\n",
    "- $\\dots$\n",
    "- $(x_K, +\\infty)$\n",
    "\n",
    "这样，我们就把连续的数据转化成了离散的取值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a1409b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age ：\n",
      "0.4200\n",
      "9.2622\n",
      "18.1044\n",
      "26.9467\n",
      "35.7889\n",
      "44.6311\n",
      "53.4733\n",
      "62.3156\n",
      "71.1578\n",
      "80.0000\n",
      "Fare ：\n",
      "0.0000\n",
      "56.9255\n",
      "113.8509\n",
      "170.7764\n",
      "227.7019\n",
      "284.6273\n",
      "341.5528\n",
      "398.4783\n",
      "455.4037\n",
      "512.3292\n"
     ]
    }
   ],
   "source": [
    "feat_ranges = {} #字典\n",
    "cont_feat = ['Age', 'Fare'] # 连续特征\n",
    "bins = 10 # 分类点数\n",
    "\n",
    "for feat in cont_feat:\n",
    "    # 数据集中存在缺省值nan，需要用np.nanmin和np.nanmax,如果直接使用 min() 或 max()，当列中存在NaN时会抛出错误或返回 NaN\n",
    "    # 计算当前特征列的最小值和最大值，忽略缺失值（NaN）。\n",
    "    min_val = np.nanmin(data[feat])   \n",
    "    max_val = np.nanmax(data[feat])   \n",
    "    feat_ranges[feat] = np.linspace(min_val, max_val, bins).tolist()  #.tolist()将NumPy数组转换为Python列表，方便后续操作。\n",
    "    print(feat, '：') # 查看分类点\n",
    "    for spt in feat_ranges[feat]:\n",
    "        print(f'{spt:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e6acc",
   "metadata": {},
   "source": [
    "#### 介绍一下分类类型\n",
    "\n",
    "分类类型是指数据的取值范围是有限的、离散的集合，而不是连续的数值。例如：\n",
    "\n",
    "- 性别 ：`['male', 'female']`\n",
    "- 颜色 ：`['red', 'green', 'blue']` \n",
    "\n",
    "\n",
    "分类类型的常见操作\n",
    "\n",
    " - 数据类型转换\n",
    "   \n",
    "   - 在 Pandas 中，可以将分类数据转换为 category 类型\n",
    "\n",
    "    ``` python\n",
    "    data['Sex'] = data['Sex'].astype('category')\n",
    "    ```\n",
    " - 查看类别\n",
    "\n",
    "   - 使用 `cat.categories` 查看所有类别：\n",
    "\n",
    "   ``` python \n",
    "   print(data['Sex'].cat.categories) \n",
    "   #输出示例\n",
    "   # Index(['female', 'male'], dtype='object')\n",
    "   ```\n",
    " \n",
    " - 编码\n",
    "   \n",
    "   - 整数编码（Label Encoding）\n",
    "     \n",
    "     - 将类别按顺序映射为整数：\n",
    "     ``` python\n",
    "     data['Sex'] = data['Sex'].cat.codes\n",
    "     #输出示例\n",
    "     #female -> 0 male -> 1\n",
    "     ```\n",
    "   \n",
    "   - 独热编码（One-Hot Encoding）\n",
    "\n",
    "    - 使用 Pandas 的 get_dummies 方法：\n",
    "     ``` python\n",
    "     data = pd.get_dummies(data, columns=['Sex'])\n",
    "     #输出示例\n",
    "     #   Sex_female  Sex_male\n",
    "     #0      0         1\n",
    "     #1      1         0\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3569e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex:Index(['female', 'male'], dtype='object')\n",
      "Pclass:Index([1, 2, 3], dtype='int64')\n",
      "SibSp:Index([0, 1, 2, 3, 4, 5, 8], dtype='int64')\n",
      "Parch:Index([0, 1, 2, 3, 4, 5, 6], dtype='int64')\n",
      "Cabin:Index(['A10', 'A14', 'A16', 'A19', 'A20', 'A23', 'A24', 'A26', 'A31', 'A32',\n",
      "       ...\n",
      "       'E8', 'F E69', 'F G63', 'F G73', 'F2', 'F33', 'F38', 'F4', 'G6', 'T'],\n",
      "      dtype='object', length=147)\n",
      "Embarked:Index(['C', 'Q', 'S'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 只有有限取值的离散特征\n",
    "cat_feat = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Cabin', 'Embarked'] \n",
    "\n",
    "for feat in cat_feat:\n",
    "    data[feat]=data[feat].astype('category') # 将特征列的数据类型转换为Pandas的category类型\n",
    "    print(f'{feat}:{data[feat].cat.categories}') #查看类别\n",
    "    data[feat]=data[feat].cat.codes.to_list() #将类别按顺序转化为整数\n",
    "    ranges=list(set(data[feat]))  #使用 set(data[feat]) 获取当前特征的所有唯一取值。\n",
    "    ranges.sort()             \n",
    "    feat_ranges[feat]=ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09b16fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将所有缺省值替换为-1\n",
    "data.fillna(-1,inplace=True)  \n",
    "for feat in feat_ranges.keys():  # 遍历字典feat_ranges中的所有键（即特征名）。\n",
    "    feat_ranges[feat]=[-1]+feat_ranges[feat] #使用列表拼接操作[-1] + feat_ranges[feat]，确保 -1 成为第一个元素。\n",
    "\n",
    "#这行代码的作用是将 -1 添加到每个特征的取值范围中，确保缺失值的填充值也被视为合法的类别之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58f4d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked'], dtype='object')\n",
      "Survived\n",
      "训练集大小: 712\n",
      "测试集大小: 179\n",
      "特征数大小: 8\n"
     ]
    }
   ],
   "source": [
    "#划分数据集与测试集\n",
    "np.random.seed(0)\n",
    "\n",
    "feat_names=data.columns[1:]\n",
    "label_name=data.columns[0]\n",
    "print(feat_names)\n",
    "print(label_name)\n",
    "\n",
    "#重排下标\n",
    "data=data.reindex(np.random.permutation(data.index))\n",
    "ratio=0.8\n",
    "split=int(ratio*len(data))\n",
    "\n",
    "train_x=data[:split].drop(columns='Survived').to_numpy()\n",
    "train_y=data['Survived'][:split].to_numpy()\n",
    "test_x=data[split:].drop(columns='Survived').to_numpy()\n",
    "test_y=data['Survived'][split:].to_numpy()\n",
    "\n",
    "print('训练集大小:',len(train_x))\n",
    "print('测试集大小:',len(test_x))\n",
    "print('特征数大小:',train_x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76d85b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        #内部结点的feat表示用来分类的特征编号，其数字与数据中的顺序对应\n",
    "        #叶节点的feat表示该结点的分类结果\n",
    "        self.feat=None\n",
    "        #分类值列表，表示按照其中的值向子结点分类\n",
    "        self.split=None\n",
    "        #子节点列表，叶节点的child为空\n",
    "        self.child=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self,X,Y,feat_ranges,lbd):\n",
    "        self.root=Node()\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "        self.feat_ranges=feat_ranges #特征取值范围\n",
    "        self.lbd=lbd  #正则化系数\n",
    "        self.eps=1e-8 #防止log0的情况\n",
    "        self.T=0  #记录叶节点数\n",
    "        self.ID3(self.root,self.X,self.Y)\n",
    "\n",
    "    #工具函数，计算 a*log a\n",
    "    def aloga(self,a):\n",
    "        return a*np.log2(a+self.eps)\n",
    "    \n",
    "    #计算某个子数据集的熵\n",
    "    def entropy(self,Y):\n",
    "        #统计每个类别出现的次数\n",
    "        #返回的是一个包含两个元素的元组：\n",
    "        # 1 唯一值  2 每个唯一值对应的出现次数。\n",
    "        cnt=np.unique(Y,return_counts=True)[1]   #return_counts=True: 表示返回每个唯一值的出现次数。 \n",
    "        N=len(Y)\n",
    "        ent=-np.sum([self.aloga(Ni/N) for Ni in cnt])\n",
    "        return ent\n",
    "    #计算feat<=val划分数据集的信息增益\n",
    "\n",
    "    def info_gain(self,X,Y,feat,val):\n",
    "        #划分前的熵\n",
    "        N=len(Y)\n",
    "        if N==0 :\n",
    "            return 0\n",
    "        HX=self.entropy(Y)\n",
    "        HXY=0 #H（X|Y）\n",
    "        #分别计算H（X|X_F<=val）和分别计算H（X|X_F>val）\n",
    "        Y_l=Y[X[:,feat]<=val]\n",
    "        HXY+=len(Y_l)/len(Y)*self.entropy(Y_l)\n",
    "        Y_r=Y[X[:,feat]>val]\n",
    "        HXY+=len(Y_r)/len(Y)*self.entropy(Y_r)\n",
    "        return HX-HXY\n",
    "    \n",
    "    #计算特征feat<=val本身的复杂度H_Y(X)\n",
    "    def entropy_YX(self,X,Y,feat,val):\n",
    "        HYX = 0\n",
    "        N = len(Y)\n",
    "        if N == 0:\n",
    "            return 0\n",
    "        Y_l = Y[X[:, feat] <= val]\n",
    "        HYX += -self.aloga(len(Y_l) / N)\n",
    "        Y_r = Y[X[:, feat] > val]\n",
    "        HYX += -self.aloga(len(Y_r) / N)\n",
    "        return HYX\n",
    "    \n",
    "    #计算用feat<=val划分数据集的信息增益率\n",
    "    def info_gain_ratio(self,X,Y,feat,val):\n",
    "        IG=self.info_gain(X,Y,feat,val)\n",
    "        HYX=self.entropy_YX(X,Y,feat,val)\n",
    "        return IG/HYX\n",
    "        \n",
    "    #用ID3算法递归分裂结点，构造决策树\n",
    "    def ID3(self,node,X,Y):\n",
    "        #判断是否已经分类完成\n",
    "        if len(np.unique(Y))==1:\n",
    "            node.feat=Y[0]\n",
    "            self.T+=1\n",
    "            return \n",
    "        \n",
    "        #寻找最优分类特征和分类点\n",
    "        best_IGR=0\n",
    "        best_feat=None\n",
    "        best_val=None\n",
    "        for feat in range(len(feat_names)):\n",
    "            for val in self.feat_ranges[feat_names[feat]]:\n",
    "                IGR=self.info_gain_ratio(X,Y,feat,val)\n",
    "                if IGR>best_IGR:\n",
    "                    best_IGR=IGR\n",
    "                    best_feat=feat\n",
    "                    best_val=val\n",
    "        #计算用best_feat<=best_val分类带来的代价函数变化\n",
    "        #由于分裂叶结点只涉及该局部，我们只需要计算分裂前后该结点的代价函数\n",
    "        #当前代价\n",
    "        cur_cost=len(Y)*self.entropy(Y)+self.lbd\n",
    "        #分裂后的代价，按best_feat的取值分裂统计\n",
    "        #如果best_feat为None，说明最优的信息增益率为0\n",
    "        #在分类也无法增加信息了，因此将new_cost设置为无穷大\n",
    "        if best_feat is None:\n",
    "            new_cost=np.inf\n",
    "        else:\n",
    "            new_cost=0\n",
    "            X_feat=X[:,best_feat]\n",
    "            #获取划分后的两部分，计算新的熵\n",
    "            new_Y_l=Y[X_feat<=best_val]\n",
    "            new_cost+=len(new_Y_l)*self.entropy(new_Y_l)\n",
    "            new_Y_r=Y[X_feat>best_val]\n",
    "            new_cost+=len(new_Y_r)*self.entropy(new_Y_r)\n",
    "            #分裂后会有两个叶节点\n",
    "            new_cost+=2*self.lbd\n",
    "        \n",
    "        if new_cost <= cur_cost:\n",
    "            # 如果分裂后代价更小，那么执行分裂\n",
    "            node.feat = best_feat\n",
    "            node.split = best_val\n",
    "            l_child = Node()\n",
    "            l_X = X[X_feat <= best_val]\n",
    "            l_Y = Y[X_feat <= best_val]\n",
    "            self.ID3(l_child, l_X, l_Y)\n",
    "            r_child = Node()\n",
    "            r_X = X[X_feat > best_val]\n",
    "            r_Y = Y[X_feat > best_val]\n",
    "            self.ID3(r_child, r_X, r_Y)\n",
    "            node.child = [l_child, r_child]\n",
    "        else:\n",
    "            # 否则将当前结点上最多的类别作为该结点的类别\n",
    "            vals, cnt = np.unique(Y, return_counts=True)\n",
    "            node.feat = vals[np.argmax(cnt)]\n",
    "            self.T += 1\n",
    "        \n",
    "    def predict(self,x):\n",
    "        node=self.root\n",
    "        #从根结点开始向下寻找，找到叶结点结束\n",
    "        while node.split is not None:\n",
    "            if x[node.feat] <= node.split:\n",
    "                node=node.child[0]\n",
    "            else:\n",
    "                node=node.child[1]\n",
    "        \n",
    "        #到达叶结点，返回类型\n",
    "        return node.feat\n",
    "    \n",
    "    #计算在样本X，标签Y上的准确率\n",
    "    def accuracy(self,X,Y):\n",
    "        correct=0\n",
    "        for x,y in zip(X,Y):\n",
    "            pred=self.predict(x)\n",
    "            if pred==y:\n",
    "                correct+=1\n",
    "        return correct/len(Y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28fef007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "叶结点数量： 23\n",
      "训练集准确率： 0.8300561797752809\n",
      "测试集准确率： 0.7262569832402235\n"
     ]
    }
   ],
   "source": [
    "DT = DecisionTree(train_x, train_y, feat_ranges, lbd=1.0)\n",
    "print('叶结点数量：', DT.T)\n",
    "\n",
    "# 计算在训练集和测试集上的准确率\n",
    "print('训练集准确率：', DT.accuracy(train_x, train_y))\n",
    "print('测试集准确率：', DT.accuracy(test_x, test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
