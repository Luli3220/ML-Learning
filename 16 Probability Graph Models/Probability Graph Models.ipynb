{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df09f2cf",
   "metadata": {},
   "source": [
    "在数据准备阶段，我们从`sklearn.datasets`中使用`fetch_20newsgroups_vectorized()`函数加载了一个经典的数据集:20 Newsgroups 数据集。\n",
    "\n",
    "这个数据集常用于文本分类、自然语言处理（NLP）和机器学习任务。\n",
    "\n",
    "## 20 Newsgroups 数据集介绍（向量化版）\n",
    "\n",
    "###  一、什么是 20 Newsgroups 数据集？\n",
    "\n",
    "- **20 Newsgroups** 是一个包含大约 20,000 篇 Usenet 新闻组文档的数据集。\n",
    "- 每个文档属于 20 个不同主题之一。\n",
    "- 常用于文本分类和自然语言处理任务。\n",
    "\n",
    "###  二、函数 `fetch_20newsgroups_vectorized()` 的作用\n",
    "\n",
    "这是 Scikit-learn 提供的一个便捷函数，用于直接加载已经**向量化（vectorized）**好的数据：\n",
    "\n",
    "- `fetch_20newsgroups()`：加载原始文本数据（字符串形式）\n",
    "- `fetch_20newsgroups_vectorized()`：加载的是已经转换为数值特征向量的形式（如 TF-IDF）\n",
    "\n",
    "> 这样可以直接用于训练模型，无需手动进行文本向量化。\n",
    "\n",
    "---\n",
    "\n",
    "###  三、代码解析\n",
    "\n",
    "```python\n",
    "train_data = fetch_20newsgroups_vectorized(subset='train',  #加载训练集（也可以是'test'或'all'）\n",
    "                                           normalize=False, #是否对特征向量进行归一化（默认不归一化）\n",
    "                                           data_home='20newsgroups') #下载或缓存数据的本地路径\n",
    "```\n",
    "\n",
    "### 四、返回对象结构（train_data / test_data）\n",
    "   - .data 特征矩阵（numpy 数组或稀疏矩阵），每一行代表一篇文章的向量化表示 \n",
    "   - .target 标签数组，每个元素是一个整数，表示文章所属的类别（0~19）\n",
    "   - .target_names 类别名称列表，对应.target的索引\n",
    "   - .DESCR 数据集描述信息（可用于查看详细说明）\n",
    "   - train_data.feature_names 每个列索引对应的原始词\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0589467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文章主题: alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n",
      "  (0, 56979)\t4\n",
      "  (0, 106171)\t2\n",
      "  (0, 129935)\t2\n",
      "  (0, 119977)\t2\n",
      "  (0, 106184)\t3\n",
      "  (0, 29279)\t3\n",
      "  (0, 111322)\t1\n",
      "  (0, 29403)\t3\n",
      "  (0, 123796)\t6\n",
      "  (0, 27618)\t1\n",
      "  (0, 92203)\t2\n",
      "  (0, 66608)\t16\n",
      "  (0, 86247)\t6\n",
      "  (0, 95392)\t2\n",
      "  (0, 101034)\t1\n",
      "  (0, 115475)\t10\n",
      "  (0, 47982)\t1\n",
      "  (0, 125053)\t3\n",
      "  (0, 76032)\t1\n",
      "  (0, 20228)\t1\n",
      "  (0, 29573)\t1\n",
      "  (0, 36979)\t1\n",
      "  (0, 40042)\t1\n",
      "  (0, 33764)\t1\n",
      "  (0, 43740)\t2\n",
      "  :\t:\n",
      "  (0, 83940)\t1\n",
      "  (0, 92260)\t1\n",
      "  (0, 81998)\t1\n",
      "  (0, 106239)\t1\n",
      "  (0, 123430)\t1\n",
      "  (0, 52449)\t1\n",
      "  (0, 117029)\t1\n",
      "  (0, 114520)\t1\n",
      "  (0, 96088)\t1\n",
      "  (0, 125017)\t1\n",
      "  (0, 53572)\t1\n",
      "  (0, 89503)\t1\n",
      "  (0, 28948)\t1\n",
      "  (0, 6214)\t1\n",
      "  (0, 109025)\t1\n",
      "  (0, 29400)\t1\n",
      "  (0, 115508)\t1\n",
      "  (0, 76685)\t1\n",
      "  (0, 53320)\t1\n",
      "  (0, 107568)\t1\n",
      "  (0, 117020)\t1\n",
      "  (0, 108951)\t1\n",
      "  (0, 104352)\t1\n",
      "  (0, 80986)\t1\n",
      "  (0, 6216)\t1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
    "from tqdm import trange\n",
    "\n",
    "#normalize 表示是否对数据归一化，这里我们保留原始数据\n",
    "#data_home 表示数据存放的路径\n",
    "\n",
    "train_data=fetch_20newsgroups_vectorized(subset='train',\n",
    "                                         normalize=False,\n",
    "                                         data_home='20newsgroups')\n",
    "test_data=fetch_20newsgroups_vectorized(subset='test',\n",
    "                                        normalize=False,\n",
    "                                        data_home='20newsgroups')\n",
    "\n",
    "print('文章主题:','\\n'.join(train_data.target_names))\n",
    "\n",
    "print(train_data.data[0]) \n",
    "\n",
    "# 介绍一下这个打印的数据集的含义\n",
    "# (0, 56979)\t4  第 0 个文档（即第一个新闻文章）,在第 56979 个特征维度上（即词汇表中的第 56979 个词）,出现了 4 次（或其对应的 TF-IDF 值为 4）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570f0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新闻数量： [480 584 591 590 578 593 585 594 598 597 600 595 591 594 593 599 546 564\n",
      " 465 377]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1787565/1787565 [00:43<00:00, 41301.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# 统计新闻主题频率\n",
    "cat_cnt = np.bincount(train_data.target)\n",
    "print('新闻数量：', cat_cnt)\n",
    "log_cat_freq = np.log(cat_cnt / np.sum(cat_cnt)) #用np.log(...)是为了防止多个小概率相乘导致下溢，使用加法代替乘法。\n",
    "\n",
    "# 对每个主题统计单词频率\n",
    "alpha = 1.0\n",
    "\n",
    "\n",
    "log_voc_freq = np.zeros((20, len(train_data.feature_names))) + alpha  \n",
    "#log_voc_freq表示每个主题下每个词的累计频率。\n",
    "#20是主题个数 train_data.feature_names是分割出的单词\n",
    "# 加上 alpha 和 len(...) * alpha就是为了防止除以零，并让未出现的词也有一个小概率,出现过的词频率页不会被影响太大。 \n",
    "# 这种做法称为拉普拉斯平滑（Laplace Smoothing）\n",
    "\n",
    "\n",
    "voc_cnt = np.zeros((20, 1)) + len(train_data.feature_names) * alpha \n",
    "#表示每个主题下所有词的总词频（加上平滑项）。\n",
    "#假设每个主题已经“虚拟地”包含了所有词各一次。\n",
    "\n",
    "\n",
    "# 用nonzero返回稀疏矩阵不为零的行列坐标\n",
    "rows, cols = train_data.data.nonzero()\n",
    "#data是一个一个稀疏矩阵，形状为 (n_samples, n_features)\n",
    "#矩阵中的值通常是词频（Count）或 TF-IDF 值\n",
    "\n",
    "for i in trange(len(rows)):\n",
    "    news = rows[i]\n",
    "    voc = cols[i]\n",
    "    cat = train_data.target[news]  #新闻类别\n",
    "    log_voc_freq[cat, voc] += train_data.data[news, voc]\n",
    "    voc_cnt[cat] += train_data.data[news, voc]\n",
    "\n",
    "log_voc_freq = np.log(log_voc_freq / voc_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4f63a2",
   "metadata": {},
   "source": [
    "### 关于先验，似然，后验\n",
    "- 先验 P(topic)    log_cat_freq   每个主题在训练集中出现的频率（取对数）\n",
    "- 似然 P(word∣topic)  log_voc_freq[:, voc] 每个词在各个主题下的条件概率（取对数）\n",
    "- 后验 P(topic∣document)  log_post 综合考虑先验和所有词的似然后，该文档属于各主题的概率（取对数）\n",
    "\n",
    "举个通俗的例子：疾病检测\n",
    " - 先验概率 P(患病) ： 人群中这种病的发病率，比如 1%。\n",
    " - 似然 P(阳性∣患病) ：如果真的患病，检测出阳性的概率（比如是 95%）\n",
    " - 后验概率 P(患病∣阳性) ：这才是你关心的 —— 看到阳性结果后，你真正患病的概率\n",
    "\n",
    "可以把它们想象成：\n",
    " \n",
    " - 先验（Prior）  医生说“这个病的发病率只有 1%”，这是他还没给你做检查时就知道的。\n",
    " - 似然（Likelihood） 检查结果显示你是阳性，这是根据你的身体数据得出的。\n",
    " - 后验（Posterior） 综合这两条信息，医生告诉你：“你实际得病的可能性大约是 16%。”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d1a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_news(news):\n",
    "    rows, cols = news.nonzero()\n",
    "    # 对数后验\n",
    "    log_post = np.copy(log_cat_freq)\n",
    "    for row, voc in zip(rows, cols):\n",
    "        # 加上每个单词在类别下的后验\n",
    "        log_post += log_voc_freq[:, voc]\n",
    "    return np.argmax(log_post)\n",
    "\n",
    "#先加上先验的目的： 当似然都相同的时候，那么不就是先验的概率高的更有可能吗\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b19c997f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类准确率： 0.7823951141795008\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for news in test_data.data:\n",
    "    preds.append(test_news(news))\n",
    "acc = np.mean(np.array(preds) == test_data.target)\n",
    "print('分类准确率：', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
